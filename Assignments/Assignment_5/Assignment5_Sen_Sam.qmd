# Assignment 5
**Sam Sen** 
**MUSA 5080** 



## Part 1: Indigo Rideshare Data: Q4 2024

### Download and Adapt Data
```{r}
library(riem)
library(tidyverse)
library(lubridate)
library(tigris)
library(sf)
library(ggplot2)

```
***We begin by loading the main packages for the analysis. tidyverse and lubridate handle data wrangling and date–time manipulation. riem provides historical weather data from airport stations. I also loaded spatial packages (tigris, sf) and ggplot2 for visualization. I then installed and attached riem to query Philadelphia weather data*** 


```{r}
indego <- read_csv("Data/indego-trips-2024-q4.csv")
glimpse(indego)

```
```{r}
colnames(indego)

```
```{r}
indego <- indego %>%
  mutate(
    # Parse datetime
    start_datetime = mdy_hm(start_time),
    end_datetime   = mdy_hm(end_time),

    # Create hourly bins
    interval60 = floor_date(start_datetime, unit = "hour"),

    # Time features
    week  = week(interval60),
    dotw  = wday(interval60, label = TRUE),
    hour  = hour(interval60),
    date  = as.Date(interval60)
  )

# Quick check
glimpse(indego)

```

**Next, we converted the trip start and end times to proper datetime objects and binned each trip into an hourly interval (interval60). I also created standard temporal features: week number, day of week, hour of day, and calendar date. These fields form the backbone of the time-series modeling**

```{r}
trips_panel <- indego %>%
  group_by(
    interval60,
    start_station,
    start_lat,
    start_lon
  ) %>%
  summarize(
    Trip_Count = n(),
    .groups = "drop"
  )

glimpse(trips_panel)

```

**I aggregated the raw trip records into a station–hour panel. For each combination of starting station, hour, and location (lat/lon), I counted the number of trips (Trip_Count). This reduces the data from individual trips to a count outcome suitable for demand modelin** 

```{r}
# How many unique stations and hours do we have?
n_stations <- length(unique(trips_panel$start_station))
n_hours    <- length(unique(trips_panel$interval60))

n_stations
n_hours

# How many rows would a complete panel have?
expected_rows <- n_stations * n_hours
expected_rows

# Build the full station-hour grid
study_panel <- expand.grid(
  interval60    = sort(unique(trips_panel$interval60)),
  start_station = sort(unique(trips_panel$start_station))
) %>%
  # Bring in the observed trip counts + lat/lon
  left_join(trips_panel, by = c("interval60", "start_station")) %>%
  # Any missing Trip_Count means 0 trips in that hour at that station
  mutate(
    Trip_Count = replace_na(Trip_Count, 0)
  )

glimpse(study_panel)

```

```{r}
# Build station attributes (one row per station)
station_attributes <- trips_panel %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    .groups = "drop"
  )

glimpse(station_attributes)

```
```{r}
# Remove the mostly-NA lat/lon columns, then join clean ones back
study_panel <- study_panel %>%
  select(-start_lat, -start_lon) %>%   # drop old ones
  left_join(station_attributes, by = "start_station")

glimpse(study_panel)

```
```{r}
study_panel <- study_panel %>%
  mutate(
    week    = week(interval60),
    month   = month(interval60, label = TRUE),
    dotw    = wday(interval60, label = TRUE),
    hour    = hour(interval60),
    date    = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )

glimpse(study_panel)

```
**I created a separate station-level table with one record per start_station, storing its latitude and longitude. I then rejoined these attributes back into the full panel so that every station–hour row has consistent, non-missing coordinates for mapping and spatial analysis** 

```{r}
# Get weather from Philly airport (KPHL) for Q4 2024
weather_data <- riem_measures(
  station    = "PHL",          # Philadelphia International Airport
  date_start = "2024-10-01",
  date_end   = "2024-12-31"
)

# Process into hourly summaries
weather_panel <- weather_data %>%
  mutate(
    interval60   = floor_date(valid, unit = "hour"),
    Temperature  = tmpf,                      # temp in F
    Precipitation = ifelse(is.na(p01i), 0, p01i),
    Wind_Speed   = sknt
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct()

glimpse(weather_panel)
summary(weather_panel$Temperature)

```
```{r}
study_panel <- study_panel %>%
  left_join(weather_panel, by = "interval60")

glimpse(study_panel)

```

**To avoid missing time periods, I constructed a complete grid of all station–hour combinations present in Q4 2024. I joined the observed trip counts onto this grid and set any missing Trip_Count values to zero. This ensures that the model sees both busy and idle hours for each station** 

```{r}
study_panel <- study_panel %>%
  arrange(start_station, interval60) %>%   # sort correctly
  group_by(start_station) %>%
  mutate(
    lag1Hour   = lag(Trip_Count, 1),
    lag2Hours  = lag(Trip_Count, 2),
    lag3Hours  = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day    = lag(Trip_Count, 24)
  ) %>%
  ungroup()

glimpse(study_panel)

```

**To capture persistence in demand, I created several lagged features for each station: trips 1, 2, and 3 hours ago, 12 hours ago, and 24 hours ago (lag1day). These lags help the model learn that current demand often depends on recent demand at the same station**

```{r}
study_panel_complete <- study_panel %>%
  filter(
    !is.na(lag1Hour),
    !is.na(lag2Hours),
    !is.na(lag3Hours),
    !is.na(lag12Hours),
    !is.na(lag1day)
  )

glimpse(study_panel_complete)

```
**Because lagged variables are undefined for the first few hours at each station, I removed rows with missing lag values. The resulting study_panel_complete starts after the first full day of data and has all lag features populated, which is necessary for modeling**

```{r}
# Temporal train/test split for Q4 2024
train <- study_panel_complete %>%
  filter(week < 50)

test <- study_panel_complete %>%
  filter(week >= 50)

# Basic checks
nrow(train)
nrow(test)

min(train$date); max(train$date)
min(test$date);  max(test$date)
```

### Model Creation
**I split the data into a training set (weeks 40–49) and a test set (weeks 50–52). This temporal split avoids leaking future information into the past and mimics a real-world forecasting scenario where we train on earlier weeks and evaluate on later weeks in the same quarter**

```{r}
# In the training data: make a clean day-of-week factor
train <- train %>%
  mutate(
    dotw_simple = factor(
      dotw,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
    )
  )

# Set dummy (treatment) coding for dotw_simple
contrasts(train$dotw_simple) <- contr.treatment(7)

# Quick check
table(train$dotw_simple)

```
**In the training data, I created a clean day-of-week factor (dotw_simple) with an explicit ordering from Monday to Sunday and set treatment/dummy coding. This makes the interpretation of day-of-week coefficients more straightforward in the linear model**

```{r}
model1 <- lm(
  Trip_Count ~ 
    as.factor(hour) +        # hour of day
    dotw_simple +            # day of week
    Temperature + 
    Precipitation,
  data = train
)

summary(model1)

```

**Model 1 is a baseline linear regression that predicts trips per station-hour using only time-of-day, day-of-week, temperature, and precipitation. This captures broad temporal and weather patterns without using any station-specific or lagged information**

**Model 2 extends the baseline by adding lagged demand features: trips 1 hour ago, 3 hours ago, and 24 hours ago. These lags allow the model to use recent station history to refine predictions, capturing short-run momentum or inertia in bike demand**
```{r}
train <- study_panel_complete %>% filter(week < 50)

```


```{r}
colnames(train)

```



```{r}
# Trip_Count ~ hour + dotw_simple + Temperature + Precipitation +
#   lag1Hour + lag3Hours + lag1day
```


```{r}
# Make sure train has the dotw_simple factor
train <- study_panel_complete %>%
  filter(week < 50) %>%
  mutate(
    dotw_simple = factor(
      dotw,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
    )
  )

contrasts(train$dotw_simple) <- contr.treatment(7)

# ---- Model 2: add lag variables ----
model2 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day,
  data = train
)

summary(model2)

```

```{r}
model4 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    as.factor(start_station),
  data = train
)

summary(model4)$adj.r.squared

```

```{r}
model5 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    as.factor(start_station) +
    rush_hour * weekend,   # interaction term
  data = train
)

summary(model5)$adj.r.squared

```

**Model 4 adds station fixed effects via as.factor(start_station). This lets each station have its own baseline level of demand, controlling for unobserved factors like land use, nearby transit, or neighborhood characteristics that are constant over time within the quarter**

**Model 5 keeps the station fixed effects and lag structure and adds an interaction between rush_hour and weekend. This allows the effect of rush-hour periods to differ between weekdays and weekends, reflecting the idea that commuting peaks may be weaker or absent on weekends**

```{r}
# Recreate dotw_simple inside test
test <- test %>%
  mutate(
    dotw_simple = factor(
      dotw,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
    )
  )

contrasts(test$dotw_simple) <- contr.treatment(7)

# Generate predictions
test <- test %>%
  mutate(
    pred1 = predict(model1, newdata = test),
    pred2 = predict(model2, newdata = test),
    pred4 = predict(model4, newdata = test),
    pred5 = predict(model5, newdata = test)
  )

head(test)

```

**I recreated the same day-of-week factor in the test set to match the training coding and then generated out-of-sample predictions from all four models (1, 2, 4, and 5). These predictions are stored as pred1, pred2, pred4, and pred5 for later error analysis**

```{r}
mae_results <- tibble(
  Model = c(
    "Model 1: Time + Weather",
    "Model 2: + Lags",
    "Model 4: + Station FE",
    "Model 5: + Interaction"
  ),
  MAE = c(
    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)
  )
)

mae_results

```

### MAE and Result Comparison 
**I evaluated each model’s predictive performance on the test weeks using mean absolute error (MAE) in trips per station-hour. Model 2 (with lags) achieved the lowest MAE, while the more complex models with station fixed effects and interactions did not further reduce test error, suggesting some overfitting.**

```{r}
test <- test %>%
  mutate(
    error_model2 = Trip_Count - pred2,
    abs_error_model2 = abs(error_model2),
    time_of_day = case_when(
      hour < 7 ~ "Overnight",
      hour >= 7 & hour < 10 ~ "AM Rush",
      hour >= 10 & hour < 15 ~ "Mid-Day",
      hour >= 15 & hour <= 18 ~ "PM Rush",
      TRUE ~ "Evening"
    ),
    weekend_label = ifelse(weekend == 1, "Weekend", "Weekday")
  )

glimpse(test)

```
**Focusing on Model 2 (the best-performing model), I computed residuals (error_model2) and absolute errors (abs_error_model2) for each station-hour in the test set. I also grouped hours into broader time-of-day buckets (Overnight, AM Rush, Mid-Day, PM Rush, Evening) and labeled each record as Weekend or Weekday to study temporal error patterns** 

```{r}
library(ggplot2)

ggplot(test, aes(x = Trip_Count, y = pred2)) +
  geom_point(alpha = 0.15, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", size = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_grid(weekend_label ~ time_of_day) +
  labs(
    title = "Observed vs. Predicted Trips (Model 2)",
    subtitle = "Faceted by Weekend/Weekday and Time of Day",
    x = "Observed Trips",
    y = "Predicted Trips"
  ) +
  theme_minimal()

```
## Part II: Error Analysis 
### Spatial Patterns
```{r}
station_errors <- test %>%
  group_by(start_station, start_lat, start_lon) %>%
  summarize(
    MAE = mean(abs_error_model2, na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  )

glimpse(station_errors)

```

**To examine spatial patterns in prediction error, I aggregated the test-set errors to the station level. For each station, I calculated the MAE of Model 2 and the average demand in the test period. This creates a station-level dataset (station_errors) that can be visualized on a map**

```{r}

options(tigris_use_cache = TRUE)

philly_boundary <- places(
  state = "PA",
  cb = TRUE,
  year = 2023
) %>%
  filter(NAME == "Philadelphia") %>%
  st_transform(4326)   # match lon/lat CRS

```
```{r}
ggplot() +
  # Philly polygon background
  geom_sf(
    data = philly_boundary,
    fill = "grey95",
    color = "white",
    linewidth = 0.3
  ) +
  # Station error points
  geom_point(
    data = station_errors,
    aes(x = start_lon, y = start_lat, color = MAE),
    size = 2,
    alpha = 0.9
  ) +
  scale_color_viridis_c(option = "plasma", direction = -1) +
  coord_sf(expand = FALSE) +
  labs(
    title = "Spatial Distribution of Prediction Errors (Model 2)",
    subtitle = "MAE per Station – Q4 2024",
    color = "MAE"
  ) +
  theme_minimal() +
  theme(
    axis.title = element_blank(),
    axis.text  = element_blank(),
    axis.ticks = element_blank()
  )

```

**Finally, I mapped Model 2’s MAE by station using each station’s longitude and latitude. Each point represents a station, colored by its mean absolute error in the test period. This “MAE map” highlights spatial clusters where the model systematically under- or over-performs, which helps identify neighborhoods where additional features or different modeling approaches may be needed.The spatial distribution of Model 2’s prediction errors reveals a clear pattern across Philadelphia. The highest MAE values occur in and around Center City and University City, where trip activity is more variable and influenced by commuting, tourism, dining, and university schedules. These areas experience sharper peaks and dips in demand, making them more difficult to predict using time, weather, and lag-based features alone.In contrast, lower MAE values appear in many outer neighborhoods such as South Philly, Northwest Philly, and parts of West Philly, where trip volumes tend to be more stable, dominated by routine local travel, or close to zero during many hours. Because these stations have more consistent patterns—or are often inactive—the model performs more accurately.Overall, the map suggests that high-activity, mixed-use areas create the greatest forecasting challenges, while residential or low-activity neighborhoods yield more predictable, lower-error demand patterns. This spatial clustering of errors points to potential opportunities for feature engineering focused on employment centers, universities, and event-driven stations**

### Temporal Patterns

```{r}
temporal_errors <- test %>%
  group_by(time_of_day, weekend_label) %>%
  summarize(
    MAE = mean(abs_error_model2, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = weekend_label)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Temporal Error Patterns (Model 2)",
    subtitle = "MAE by Time of Day and Weekday/Weekend",
    x = "Time of Day",
    y = "Mean Absolute Error",
    fill = "Day Type"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
**The temporal error analysis shows that Model 2 performs unevenly across different times of day. Prediction errors are highest during the AM and PM rush periods, particularly on weekdays, when commuting patterns create sharp spikes in demand that are harder for the model to capture. Mid-day and evening periods show moderate error levels, reflecting more stable leisure and errand travel.Overnight hours have the lowest errors for both weekdays and weekends because most stations have near-zero demand during this period, making predictions easier. The fact that weekend rush periods exhibit lower MAE than weekday rush periods suggests that weekend bike usage is less peaky, more uniform, and therefore easier to forecast. Overall, the model struggles most during high-volume, high-variability commuting hours.**

```{r}
study_panel_complete <- study_panel_complete %>%
  mutate(
    # Feature 1: Feels-like temperature
    feels_like = Temperature - 0.1 * Wind_Speed,

    # Feature 2: Nice-weather indicator
    nice_weather = ifelse(Temperature >= 50 & Precipitation == 0, 1, 0)
  )

```

```{r}
train <- study_panel_complete %>%
  filter(week < 50) %>%
  mutate(
    dotw_simple = factor(
      dotw,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
    )
  )

contrasts(train$dotw_simple) <- contr.treatment(7)

test <- study_panel_complete %>%
  filter(week >= 50) %>%
  mutate(
    dotw_simple = factor(
      dotw,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
    )
  )

contrasts(test$dotw_simple) <- contr.treatment(7)

```

```{r}
model6 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    feels_like +
    nice_weather,
  data = train
)

summary(model6)

```
```{r}
test <- test %>%
  mutate(
    pred6 = predict(model6, newdata = test)
  )

mae_model6 <- mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE)
mae_model6

```

**I added two new weather-based features to Model 2:(1) Feels-like temperature, capturing wind chill effects, and (2) A “nice-weather” indicator for hours with temperature ≥ 50°F and zero precipitation. After refitting the model (Model 6), the resulting MAE on the Q4 2024 test set was 0.3869, virtually the same as Model 2’s MAE of 0.3860. This suggests that in Q4, when most hours are cold and have low, stable demand, these additional features do not meaningfully improve predictive accuracy.In warmer quarters these features might be more effective, but for Q4 they add little explanatory power because weather conditions vary less and demand is low or zero in many station-hours** 
```{r}

```


I added two new weather-based features to Model 2:
(1) Feels-like temperature, capturing wind chill effects, and
(2) A “nice-weather” indicator for hours with temperature ≥ 50°F and zero precipitation.

After refitting the model (Model 6), the resulting MAE on the Q4 2024 test set was 0.3869, virtually the same as Model 2’s MAE of 0.3860. This suggests that in Q4, when most hours are cold and have low, stable demand, these additional features do not meaningfully improve predictive accuracy.

In warmer quarters these features might be more effective, but for Q4 they add little explanatory power because weather conditions vary less and demand is low or zero in many station-hours.

---
