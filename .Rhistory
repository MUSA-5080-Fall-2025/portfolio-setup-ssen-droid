grid_with_group <- st_join(grid_cent, cook_tracts, left = TRUE)
# 4️⃣ Copy GEOID (the tract ID) into your grid object as group_id
grid_agg_3857$group_id <- grid_with_group$GEOID
# 5️⃣ Sanity checks
cat("Unique tracts assigned:", length(unique(na.omit(grid_agg_3857$group_id))), "\n")
cat("Unassigned cells:", sum(is.na(grid_agg_3857$group_id)), "\n")
# Part 5 — Step 2: define function for one iteration of LOGO CV
library(MASS)     # for glm.nb
library(Metrics)  # for mae() and rmse()
install.packages("Metrics")
# Part 5 — Step 2: define function for one iteration of LOGO CV
library(MASS)     # for glm.nb
library(Metrics)  # for mae() and rmse()
run_logo <- function(test_group, data) {
# Split data into training and testing based on group_id
train_data <- data[data$group_id != test_group, ]
test_data  <- data[data$group_id == test_group, ]
# Fit Negative Binomial model (use m_pois if you prefer Poisson)
model <- glm.nb(count ~ lag5_mean + dist_to_hotspot_km, data = train_data)
# Predict for held-out group
preds <- predict(model, newdata = test_data, type = "response")
# Calculate error metrics for that group
data.frame(
group_id = test_group,
MAE  = mae(test_data$count, preds),
RMSE = rmse(test_data$count, preds)
)
}
# Part 5 — Step 2: define function for one iteration of LOGO CV
library(MASS)     # for glm.nb
library(Metrics)  # for mae() and rmse()
run_logo <- function(test_group, data) {
# Split data into training and testing based on group_id
train_data <- data[data$group_id != test_group, ]
test_data  <- data[data$group_id == test_group, ]
# Fit Negative Binomial model (use m_pois if you prefer Poisson)
model <- glm.nb(count ~ lag5_mean + dist_to_hotspot_km, data = train_data)
# Predict for held-out group
preds <- predict(model, newdata = test_data, type = "response")
# Calculate error metrics for that group
data.frame(
group_id = test_group,
MAE  = mae(test_data$count, preds),
RMSE = rmse(test_data$count, preds)
)
}
# Part 5 — Step 2: define function for one iteration of LOGO CV
library(MASS)     # for glm.nb
library(Metrics)  # for mae() and rmse()
run_logo <- function(test_group, data) {
# Split data into training and testing based on group_id
train_data <- data[data$group_id != test_group, ]
test_data  <- data[data$group_id == test_group, ]
# Fit Negative Binomial model (use m_pois if you prefer Poisson)
model <- glm.nb(count ~ lag5_mean + dist_to_hotspot_km, data = train_data)
# Predict for held-out group
preds <- predict(model, newdata = test_data, type = "response")
# Calculate error metrics for that group
data.frame(
group_id = test_group,
MAE  = mae(test_data$count, preds),
RMSE = rmse(test_data$count, preds)
)
}
# Prepare list of groups (drop the NAs you saw earlier)
groups <- unique(na.omit(grid_agg_3857$group_id))
# Try the first 15 groups to verify the pipeline
test_groups <- groups[1:15]
# Run LOGO for these groups
cv_results_small <- do.call(
rbind,
lapply(test_groups, run_logo, data = grid_agg_3857)
)
# Peek at results + quick averages
head(cv_results_small)
mean(cv_results_small$MAE, na.rm = TRUE)
mean(cv_results_small$RMSE, na.rm = TRUE)
# Check how many grid cells and total counts per tract
group_summary <- grid_agg_3857 %>%
st_drop_geometry() %>%
group_by(group_id) %>%
summarise(
n_cells = n(),
total_count = sum(count)
)
head(group_summary, 15)
summary(group_summary$total_count)
# Check how many tracts have fewer than 5 grid cells
group_summary <- grid_agg_3857 %>%
st_drop_geometry() %>%
group_by(group_id) %>%
summarise(n_cells = n()) %>%
arrange(n_cells)
# Count and view
small_tracts <- group_summary %>% filter(n_cells < 5)
cat("Number of tracts with fewer than 5 grid cells:", nrow(small_tracts), "\n")
# Optional: inspect them
head(small_tracts)
library(dplyr)
# Build a clean summary: how many grid cells per tract (drop NAs)
group_summary <- grid_agg_3857 |>
st_drop_geometry() |>
filter(!is.na(group_id)) |>
count(group_id, name = "n_cells")  # n_cells per tract
# 1) See how many tracts have 1, 2, 3, 4, ... cells
size_dist <- group_summary |>
count(n_cells, name = "num_tracts") |>
arrange(n_cells)
print(size_dist)
# 2) Extract ALL tracts with fewer than 5 cells (1–4), not just 1
small_tracts <- group_summary |>
filter(n_cells < 5) |>
arrange(desc(n_cells))
cat("Tracts with < 5 cells:", nrow(small_tracts), "\n")
head(small_tracts, 10)
# Part 5 — Step 2: define function for one iteration of LOGO CV
library(MASS)     # for glm.nb
library(Metrics)  # for mae() and rmse()
run_logo <- function(test_group, data) {
# Split data into training and testing based on group_id
train_data <- data[data$group_id != test_group, ]
test_data  <- data[data$group_id == test_group, ]
# Fit Negative Binomial model (use m_pois if you prefer Poisson)
model <- glm.nb(count ~ lag5_mean + dist_to_hotspot_km, data = train_data)
# Predict for held-out group
preds <- predict(model, newdata = test_data, type = "response")
# Calculate error metrics for that group
data.frame(
group_id = test_group,
MAE  = mae(test_data$count, preds),
RMSE = rmse(test_data$count, preds)
)
}
# Part 5 — Step 3: choose valid test groups (tracts with enough cells)
library(dplyr)
# cells per tract
group_summary <- grid_agg_3857 |>
st_drop_geometry() |>
filter(!is.na(group_id)) |>
count(group_id, name = "n_cells")
# distribution of tract sizes (sanity check)
size_dist <- group_summary |>
count(n_cells, name = "num_tracts") |>
arrange(n_cells)
print(size_dist)
# pick tracts with >= 5 cells as testable groups
valid_groups <- group_summary |>
filter(n_cells >= 5) |>
pull(group_id)
# dataset restricted to those groups for testing (training will still see ALL groups in each fold)
grid_valid <- grid_agg_3857 |>
filter(group_id %in% valid_groups)
# quick coverage stats
cat("Total tracts:", nrow(group_summary), "\n")
cat("Valid test tracts (>=5 cells):", length(valid_groups), "\n")
cat("Cells in valid test tracts:", nrow(grid_valid), "of", nrow(grid_agg_3857), "\n")
cat("Coverage of cells for testing:", round(100 * nrow(grid_valid)/nrow(grid_agg_3857), 1), "%\n")
# Part 5 — Step 4: quick sanity run on 10 valid tracts
test_groups <- valid_groups[1:10]
cv_results_small <- do.call(
rbind,
lapply(test_groups, run_logo, data = grid_valid)
)
# View first few results and overall averages
head(cv_results_small)
mean(cv_results_small$MAE, na.rm = TRUE)
mean(cv_results_small$RMSE, na.rm = TRUE)
# Part 5 — Step 5: full Leave-One-Group-Out CV over all valid tracts
cv_results <- do.call(
rbind,
lapply(valid_groups, run_logo, data = grid_valid)
)
# Save + summarize
head(cv_results)
mean(cv_results$MAE, na.rm = TRUE)
mean(cv_results$RMSE, na.rm = TRUE)
# Optional: save results for Part 6 comparison
saveRDS(cv_results, "Assignments/Assignment_4/Data/cv_results_part5.rds")
# Part 5 — Step 5: full Leave-One-Group-Out CV over all valid tracts
cv_results <- do.call(
rbind,
lapply(valid_groups, run_logo, data = grid_valid)
)
# Save + summarize
head(cv_results)
mean(cv_results$MAE, na.rm = TRUE)
mean(cv_results$RMSE, na.rm = TRUE)
# Part 5 — Optional diagnostic: visualize CV errors by tract
library(dplyr)
library(ggplot2)
library(viridis)
# 1️⃣  Summarize mean MAE per tract (from cv_results)
cv_summary <- cv_results |>
group_by(group_id) |>
summarise(MAE = mean(MAE, na.rm = TRUE),
RMSE = mean(RMSE, na.rm = TRUE))
# 2️⃣  Join CV errors back to the tract geometry
tracts_errors <- cook_tracts |>
left_join(cv_summary, by = c("GEOID" = "group_id"))
# 3️⃣  Plot MAE across tracts
ggplot(tracts_errors) +
geom_sf(aes(fill = MAE), color = NA) +
scale_fill_viridis_c(option = "C", name = "Mean Absolute Error") +
labs(
title = "Cross-Validated MAE by Census Tract",
subtitle = "Leave-One-Group-Out CV (Negative Binomial Model)",
caption  = "Source: City of Chicago 311 Rodent Baiting Data, 2017"
) +
theme_void() +
theme(legend.position = "right")
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
installed.packages("spatstat.geom")
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
install.packages(c("spatstat.geom", "spatstat.core", "spatstat.data"))
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
library(spatstat.core)   # density for ppp
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
library(spatstat.core)   # density for ppp
install.packages(c("spatstat.geom", "spatstat.core", "spatstat.data"))
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
library(spatstat.core)   # density for ppp
install.packages("spatstat")
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat.geom)   # ppp objects
library(spatstat.core)   # density for ppp
# Part 6 — Step 1: prepare point pattern for KDE (one-time setup)
# Packages
library(sf)
library(dplyr)
library(spatstat)
# 1) Make sure you have 2017 rodent POINTS in 3857
#    Replace 'rodent_sf_2017' with your points object name if different.
#    If your points are still in WGS84, project them first.
stopifnot(inherits(rodent_sf_2017, "sf"))
ls()
# Rename your existing points so the KDE code can find them
rodent_sf_2017 <- rodent_sf   # you listed this in ls()
# Make a 3857 boundary from your fishnet (works well for KDE window)
# Assumes grid_agg_3857 exists (your fishnet aggregated grid)
library(sf)
chi_boundary_3857 <- st_as_sf(st_make_valid(st_union(st_geometry(grid_agg_3857))))
# ---- KDE setup (re-run this after installing spatstat packages) ----
library(spatstat.geom)
library(spatstat.core)
install.packages("spatstat")
# Rename your existing points so the KDE code can find them
rodent_sf_2017 <- rodent_sf   # you listed this in ls()
# Make a 3857 boundary from your fishnet (works well for KDE window)
# Assumes grid_agg_3857 exists (your fishnet aggregated grid)
library(sf)
chi_boundary_3857 <- st_as_sf(st_make_valid(st_union(st_geometry(grid_agg_3857))))
# ---- KDE setup (re-run this after installing spatstat packages) ----
library(spatstat.geom)
library(spatstat.core)
# Rename your existing points so the KDE code can find them
rodent_sf_2017 <- rodent_sf   # you listed this in ls()
# Make a 3857 boundary from your fishnet (works well for KDE window)
# Assumes grid_agg_3857 exists (your fishnet aggregated grid)
library(sf)
chi_boundary_3857 <- st_as_sf(st_make_valid(st_union(st_geometry(grid_agg_3857))))
# ---- KDE setup (re-run this after installing spatstat packages) ----
library(spatstat.geom)
library(spatstat.core)
# --- Part 6: KDE baseline ---
# Step 1: Setup (packages, points, window, ppp)
# 1) Install & load (umbrella package is easiest)
#    If already installed, you can skip the install line.
# install.packages("spatstat")
library(spatstat)         # brings in spatstat.geom/core/etc.
library(sf)
library(dplyr)
# 2) Point data (use your existing 2017 points)
#    You told me you have `rodent_sf`. If your 2017 points live in `rodent_sf`, just alias:
rodent_sf_2017 <- rodent_sf
# 3) Make sure points are planar (EPSG:3857)
rodent_pts_3857 <- if (sf::st_is_longlat(rodent_sf_2017)) {
sf::st_transform(rodent_sf_2017, 3857)
} else {
rodent_sf_2017
}
# 4) Study window (boundary) in 3857
#    Easiest robust choice is to union your existing fishnet (same extent you’ve been modeling)
#    If you already have chi boundary in 3857 (e.g., chi_city), you can set chi_boundary_3857 <- chi_city
chi_boundary_3857 <- sf::st_as_sf(sf::st_make_valid(sf::st_union(sf::st_geometry(grid_agg_3857))))
# 5) Convert boundary to spatstat window
win_3857 <- as.owin(sf::st_as_sfc(sf::st_union(chi_boundary_3857)))
# --- Part 6: KDE baseline ---
# Step 1: Setup (packages, points, boundary, window, ppp)
# 1) Load required packages
# install.packages("spatstat")  # run once if not yet installed
library(spatstat)   # brings in spatstat.geom/core/etc.
library(sf)
library(dplyr)
# 2) Use your existing 2017 rodent points
rodent_sf_2017 <- rodent_sf
# 3) Make sure the points are projected to meters (EPSG:3857)
rodent_pts_3857 <- if (sf::st_is_longlat(rodent_sf_2017)) {
sf::st_transform(rodent_sf_2017, 3857)
} else {
rodent_sf_2017
}
# 4) Create a boundary polygon from your grid
boundary_sfc <- sf::st_union(sf::st_geometry(grid_agg_3857))   # returns sfc_MULTIPOLYGON
boundary_sfc <- sf::st_make_valid(boundary_sfc)
# 5) Convert boundary to a spatstat window
win_3857 <- spatstat.geom::as.owin(boundary_sfc)
# 6) Convert the rodent points into a spatstat ppp (point pattern)
coords <- sf::st_coordinates(rodent_pts_3857)
pp <- spatstat.geom::ppp(x = coords[,1], y = coords[,2], window = win_3857)
# 7) Sanity check outputs
cat("Point pattern:", pp$n, "events in window\n")
cat("Window area (sq. km):", round(spatstat.geom::area.owin(win_3857) / 1e6, 1), "\n")
# --- Part 6: KDE baseline ---
# Step 2: choose bandwidth, compute KDE, extract to grid, score
library(spatstat)
library(sf)
library(dplyr)
library(Metrics)  # for mae(), rmse()
# pp  = your point pattern (from prior step)
# grid_agg_3857 = your fishnet with 'count' column (observed)
# 1) Automatic bandwidth selection (Diggle’s method is a good default)
bw <- bw.diggle(pp)   # you can also try: bw.ppl(pp) or bw.scott(pp)
cat("Chosen bandwidth (meters):", round(bw, 1), "\n")
# 2) KDE intensity image (events per square meter)
#    edge=TRUE applies edge correction; leaveat defaults to pixel grid
dens_im <- density.ppp(pp, sigma = bw, edge = TRUE)
# 3) Get grid centroids (to sample the KDE at each cell)
grid_cent <- st_centroid(grid_agg_3857)
xy <- st_coordinates(grid_cent)
# 4) Extract KDE intensity at each centroid using nearest pixel lookup
np   <- spatstat.geom::nearest.pixel(xy[,1], xy[,2], dens_im)
kde_intensity <- dens_im$v[cbind(np$row, np$col)]   # units: events per m^2
# 5) Convert intensity (events/m^2) to expected count per grid cell
#    Cell area in m^2 (assumes square fishnet in EPSG:3857)
cell_area_m2 <- as.numeric(st_area(grid_agg_3857$geometry[1]))
# --- Part 6: KDE baseline ---
# Step 2: choose bandwidth, compute KDE, extract to grid, score
library(spatstat)
library(sf)
library(dplyr)
library(Metrics)  # for mae(), rmse()
# pp  = your point pattern (from prior step)
# grid_agg_3857 = your fishnet with 'count' column (observed)
# 1) Automatic bandwidth selection (Diggle’s method is a good default)
bw <- bw.diggle(pp)   # you can also try: bw.ppl(pp) or bw.scott(pp)
cat("Chosen bandwidth (meters):", round(bw, 1), "\n")
# 2) KDE intensity image (events per square meter)
#    edge=TRUE applies edge correction; leaveat defaults to pixel grid
dens_im <- density.ppp(pp, sigma = bw, edge = TRUE)
# 3) Get grid centroids (to sample the KDE at each cell)
grid_cent <- st_centroid(grid_agg_3857)
xy <- st_coordinates(grid_cent)
# 4) Extract KDE intensity at each centroid using nearest pixel lookup
np   <- spatstat.geom::nearest.pixel(xy[,1], xy[,2], dens_im)
kde_intensity <- dens_im$v[cbind(np$row, np$col)]   # units: events per m^2
# 5) Convert intensity (events/m^2) to expected count per grid cell
#    Cell area in m^2 (assumes square fishnet in EPSG:3857)
cell_area_m2 <- mean(as.numeric(st_area(grid_agg_3857)))
kde_pred_count <- pmax(0, kde_intensity * cell_area_m2)  # nonnegative
# 6) Score the KDE baseline against observed counts
obs <- grid_agg_3857$count
kde_mae  <- mae(obs, kde_pred_count)
kde_rmse <- rmse(obs, kde_pred_count)
cat("KDE baseline — MAE:", round(kde_mae, 2), "\n")
cat("KDE baseline — RMSE:", round(kde_rmse, 2), "\n")
# (Optional) Attach to grid for mapping later
grid_agg_3857$kde_pred <- kde_pred_count
library(tidyverse)
library(janitor)
rodent <- readr::read_csv("Data/Rodent.csv") %>%
janitor::clean_names()
glimpse(rodent)
nrow(rodent)
library(sf)
library(dplyr)
# 1️⃣ Separate the hot spot cells
hotspots <- grid_agg_3857 %>%
filter(quad == "High-High (Hot Spot)")
# 2️⃣ Calculate distance from every cell centroid to nearest hot spot centroid
centroids <- st_centroid(grid_agg_3857)
hot_centroids <- st_centroid(hotspots)
# st_distance creates a distance matrix (each cell vs each hot spot)
dist_matrix <- st_distance(centroids, hot_centroids)
# 3️⃣ Take the minimum distance per cell (convert from meters to km)
grid_agg_3857$dist_to_hotspot_km <- apply(dist_matrix, 1, min) / 1000
library(ggplot2)
library(viridis)
ggplot(grid_agg_3857) +
geom_sf(aes(fill = dist_to_hotspot_km), color = NA) +
scale_fill_viridis_c(name = "Distance to nearest hot spot (km)", trans = "sqrt") +
labs(
title = "Distance to Nearest Hot Spot — Rodent Baiting (2017)",
subtitle = "Each 500 m cell colored by distance from significant hot spots",
caption = "Source: City of Chicago 311; Hot spots from Local Moran’s I"
) +
theme_void() +
theme(legend.position = "right")
# --- Local Moran’s I: Hot/Cold Spot Analysis ---
library(spdep)
library(dplyr)
library(ggplot2)
#Compute Local Moran’s I for the counts
locI <- localmoran(grid_agg_3857$count, lw5, zero.policy = TRUE)
#Attach results
grid_agg_3857 <- grid_agg_3857 %>%
mutate(
Ii = locI[, 1],          # Local Moran’s I value
Ei = locI[, 2],          # Expected value
Vi = locI[, 3],          # Variance
Z  = (Ii - Ei) / sqrt(Vi), # Z-score
Pr = locI[, 5]           # p-value
)
#Classify each cell (significant clusters/outliers)
z_count <- as.numeric(scale(grid_agg_3857$count))
z_lag   <- as.numeric(scale(grid_agg_3857$lag5_mean))
sig     <- grid_agg_3857$Pr < 0.05
grid_agg_3857$quad <- dplyr::case_when(
z_count >= 0 & z_lag >= 0 & sig ~ "High-High (Hot Spot)",
z_count <  0 & z_lag <  0 & sig ~ "Low-Low (Cold Spot)",
z_count >= 0 & z_lag <  0 & sig ~ "High-Low (Outlier)",
z_count <  0 & z_lag >= 0 & sig ~ "Low-High (Outlier)",
TRUE ~ "Not Significant"
)
#Plot hot/cold spots
ggplot(grid_agg_3857) +
geom_sf(aes(fill = quad), color = NA) +
scale_fill_manual(
values = c(
"High-High (Hot Spot)"  = "#b2182b",
"Low-Low (Cold Spot)"   = "#2166ac",
"High-Low (Outlier)"    = "#ef8a62",
"Low-High (Outlier)"    = "#67a9cf",
"Not Significant"       = "grey90"
),
name = "Local Moran’s I"
) +
labs(
title    = "Local Moran’s I — Rodent Baiting Requests (2017)",
subtitle = "Significant clusters (p < 0.05) using k = 5 nearest neighbors",
caption  = "Source: City of Chicago 311"
) +
theme_void() +
theme(legend.position = "right")
AIC(m_pois, m_nb)
AIC(m_pois, m_nb)
# install.packages("MASS")  # if not installed
library(MASS)
library(broom)
m_nb <- glm.nb(count ~ lag5_mean + dist_to_hotspot_km, data = grid_mod)
summary(m_nb)
# Poisson regression with log link (default)
m_pois <- glm(
count ~ lag5_mean + dist_to_hotspot_km,
data = grid_mod,
family = poisson(link = "log")
)
summary(m_pois)
# Poisson regression with log link (default)
m_pois <- glm(
count ~ lag5_mean + dist_to_hotspot_km,
data = grid_mod,
family = poisson(link = "log")
)
summary(m_pois)
# Part 5 — Optional diagnostic: visualize CV errors by tract
library(dplyr)
library(ggplot2)
library(viridis)
# 1️⃣  Summarize mean MAE per tract (from cv_results)
cv_summary <- cv_results |>
group_by(group_id) |>
summarise(MAE = mean(MAE, na.rm = TRUE),
RMSE = mean(RMSE, na.rm = TRUE))
# 2️⃣  Join CV errors back to the tract geometry
tracts_errors <- cook_tracts |>
left_join(cv_summary, by = c("GEOID" = "group_id"))
# 3️⃣  Plot MAE across tracts
ggplot(tracts_errors) +
geom_sf(aes(fill = MAE), color = NA) +
scale_fill_viridis_c(option = "C", name = "Mean Absolute Error") +
labs(
title = "Cross-Validated MAE by Census Tract",
subtitle = "Leave-One-Group-Out CV (Negative Binomial Model)",
caption  = "Source: City of Chicago 311 Rodent Baiting Data, 2017"
) +
theme_void() +
theme(legend.position = "right")
quarto render "Assignments/Assignment_4/Assignment4_Sen_Sam.qmd"
cd "C:\Users\16468\OneDrive - PennO365\Documents\Academics\MUSA\Public_Policy_Analytics\portfolio-setup-ssen-droid"
