[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nI am originally from New York City and have worked in commercial real estate asset management for the last 5 years. I am currently a dual degree student at Penn (MBA/MUSA) and am taking Public Policy Analytics as a core requirement for the dual degree.\n\n\n\n\nEmail: [sen1@wharton.upenn.edu]\nGitHub: [@ssen-droid]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "I am originally from New York City and have worked in commercial real estate asset management for the last 5 years. I am currently a dual degree student at Penn (MBA/MUSA) and am taking Public Policy Analytics as a core requirement for the dual degree."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [sen1@wharton.upenn.edu]\nGitHub: [@ssen-droid]"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html",
    "href": "Assignments/Assignment_1/assignment_1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New York Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#scenario",
    "href": "Assignments/Assignment_1/assignment_1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the New York Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#learning-objectives",
    "href": "Assignments/Assignment_1/assignment_1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#submission-instructions",
    "href": "Assignments/Assignment_1/assignment_1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#data-retrieval",
    "href": "Assignments/Assignment_1/assignment_1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\n\n\n# ---- 2.1 County-level data (ACS 2022 5-year) ----\nvars &lt;- c(\n  med_income = \"B19013_001\",  # Median household income\n  total_pop  = \"B01003_001\"   # Total population\n)\n\ncounty_raw &lt;- get_acs(\n  geography = \"county\",\n  variables = vars,\n  year = 2022,\n  survey = \"acs5\",\n  state = my_state,\n  output = \"wide\"\n)\n\ncounty &lt;- county_raw %&gt;%\n  mutate(\n    county = NAME %&gt;%\n      stringr::str_remove(\",\\\\s*.*$\") %&gt;%  # drop \", State\"\n      stringr::str_remove(\"\\\\s*County$\")   # drop trailing \"County\"\n  ) %&gt;%\n  select(\n    county, GEOID,\n    med_incomeE, med_incomeM,\n    total_popE,  total_popM\n  )\n\nknitr::kable(\n  head(county, 10),\n  caption = paste(\"County-level ACS (2022 5-year) for\", my_state),\n  digits = 0\n)\n\n\nCounty-level ACS (2022 5-year) for New York\n\n\ncounty\nGEOID\nmed_incomeE\nmed_incomeM\ntotal_popE\ntotal_popM\n\n\n\n\nAlbany\n36001\n78829\n2049\n315041\nNA\n\n\nAllegany\n36003\n58725\n1965\n47222\nNA\n\n\nBronx\n36005\n47036\n890\n1443229\nNA\n\n\nBroome\n36007\n58317\n1761\n198365\nNA\n\n\nCattaraugus\n36009\n56889\n1778\n77000\nNA\n\n\nCayuga\n36011\n63227\n2736\n76171\nNA\n\n\nChautauqua\n36013\n54625\n1754\n127440\nNA\n\n\nChemung\n36015\n61358\n2475\n83584\nNA\n\n\nChenango\n36017\n61741\n2526\n47096\nNA\n\n\nClinton\n36019\n67097\n2802\n79839\nNA"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#data-quality-assessment",
    "href": "Assignments/Assignment_1/assignment_1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\n\n# ---- 2.2A: compute MOE% for median income ----\ncounty_moe &lt;- county %&gt;%\n  dplyr::mutate(\n    income_moe_pct = dplyr::if_else(\n      !is.na(med_incomeE) & med_incomeE &gt; 0 & !is.na(med_incomeM),\n      100 * med_incomeM / med_incomeE,\n      NA_real_\n    )\n  )\n\n# peek at just the columns we care about\nknitr::kable(\n  county_moe %&gt;%\n    dplyr::select(county, med_incomeE, med_incomeM, income_moe_pct) %&gt;%\n    head(10),\n  caption = \"MOE% for median income (first 10 counties)\",\n  digits = c(0, 0, 0, 1)\n)\n\n\nMOE% for median income (first 10 counties)\n\n\ncounty\nmed_incomeE\nmed_incomeM\nincome_moe_pct\n\n\n\n\nAlbany\n78829\n2049\n2.6\n\n\nAllegany\n58725\n1965\n3.3\n\n\nBronx\n47036\n890\n1.9\n\n\nBroome\n58317\n1761\n3.0\n\n\nCattaraugus\n56889\n1778\n3.1\n\n\nCayuga\n63227\n2736\n4.3\n\n\nChautauqua\n54625\n1754\n3.2\n\n\nChemung\n61358\n2475\n4.0\n\n\nChenango\n61741\n2526\n4.1\n\n\nClinton\n67097\n2802\n4.2\n\n\n\n\n\n\n# ---- 2.2B: add reliability category ----\ncounty_reliability &lt;- county_moe %&gt;%\n  dplyr::mutate(\n    reliability = dplyr::case_when(\n      is.na(income_moe_pct) ~ \"Unavailable\",\n      income_moe_pct &lt; 5    ~ \"High Confidence\",\n      income_moe_pct &lt;= 10  ~ \"Moderate Confidence\",\n      income_moe_pct &gt; 10   ~ \"Low Confidence\"\n    )\n  )\n\n# quick preview\nknitr::kable(\n  county_reliability %&gt;%\n    dplyr::select(county, med_incomeE, med_incomeM, income_moe_pct, reliability) %&gt;%\n    head(10),\n  caption = \"Income MOE% + reliability category (first 10)\",\n  digits = c(0, 0, 0, 1)\n)\n\n\nIncome MOE% + reliability category (first 10)\n\n\n\n\n\n\n\n\n\ncounty\nmed_incomeE\nmed_incomeM\nincome_moe_pct\nreliability\n\n\n\n\nAlbany\n78829\n2049\n2.6\nHigh Confidence\n\n\nAllegany\n58725\n1965\n3.3\nHigh Confidence\n\n\nBronx\n47036\n890\n1.9\nHigh Confidence\n\n\nBroome\n58317\n1761\n3.0\nHigh Confidence\n\n\nCattaraugus\n56889\n1778\n3.1\nHigh Confidence\n\n\nCayuga\n63227\n2736\n4.3\nHigh Confidence\n\n\nChautauqua\n54625\n1754\n3.2\nHigh Confidence\n\n\nChemung\n61358\n2475\n4.0\nHigh Confidence\n\n\nChenango\n61741\n2526\n4.1\nHigh Confidence\n\n\nClinton\n67097\n2802\n4.2\nHigh Confidence\n\n\n\n\n\n\n# ---- 2.2C: unreliable flag + summary table ----\ncounty_reliability &lt;- county_reliability %&gt;%\n  dplyr::mutate(unreliable = income_moe_pct &gt; 10)\n\nrel_summary &lt;- county_reliability %&gt;%\n  dplyr::count(reliability) %&gt;%\n  dplyr::mutate(pct = round(100 * n / sum(n), 1)) %&gt;%\n  dplyr::arrange(match(reliability, c(\"High Confidence\",\"Moderate Confidence\",\"Low Confidence\",\"Unavailable\")))\n\nknitr::kable(rel_summary, caption = \"County reliability categories\", digits = 1)\n\n\nCounty reliability categories\n\n\nreliability\nn\npct\n\n\n\n\nHigh Confidence\n56\n90.3\n\n\nModerate Confidence\n5\n8.1\n\n\nLow Confidence\n1\n1.6"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#high-uncertainty-counties",
    "href": "Assignments/Assignment_1/assignment_1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\n\n# ---- 2.3: Top 5 counties by income MOE% ----\ntop5_uncertainty &lt;- county_reliability %&gt;%\n  dplyr::filter(!is.na(income_moe_pct)) %&gt;%                 # ignore rows where MOE% couldn't be computed\n  dplyr::arrange(dplyr::desc(income_moe_pct)) %&gt;%           # highest MOE% first\n  dplyr::slice(1:5) %&gt;%                                     # top 5\n  dplyr::transmute(                                         # select + rename for presentation\n    county,\n    `Median income ($)` = med_incomeE,\n    `Income MOE ($)`    = med_incomeM,\n    `Income MOE %`      = sprintf(\"%.1f%%\", income_moe_pct),\n     Reliability        = reliability\n  )\n\nknitr::kable(\n  top5_uncertainty,\n  caption = \"Top 5 counties by median income MOE% (ACS 2022 5-year)\"\n) %&gt;%\n  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 5 counties by median income MOE% (ACS 2022 5-year)\n\n\ncounty\nMedian income ($)\nIncome MOE ($)\nIncome MOE %\nReliability\n\n\n\n\nHamilton\n66891\n7622\n11.4%\nLow Confidence\n\n\nSchuyler\n61316\n5818\n9.5%\nModerate Confidence\n\n\nGreene\n70294\n4341\n6.2%\nModerate Confidence\n\n\nYates\n63974\n3733\n5.8%\nModerate Confidence\n\n\nEssex\n68090\n3590\n5.3%\nModerate Confidence\n\n\n\n\n\nData Quality Commentary:\n[The counties that have higher % reliability are likely to benefit less from algorithmic decision making. These margins of error are too large to give us high confidence in the median income figures. Therefore, algorithms that use this data may cause harm. For example, if the state decides that counties that have &lt;$60K median income will receive social services, Hamilton looks like it’s well above this threshold but with such a high margin of error this may not be the case.]"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#focus-area-selection",
    "href": "Assignments/Assignment_1/assignment_1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\ncounty_reliability %&gt;%\n  dplyr::select(county, med_incomeE, income_moe_pct, reliability) %&gt;%\n  dplyr::arrange(income_moe_pct)\n\n# A tibble: 62 × 4\n   county      med_incomeE income_moe_pct reliability    \n   &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;          \n 1 Queens            82431           1.06 High Confidence\n 2 Suffolk          122498           1.18 High Confidence\n 3 Erie              68014           1.18 High Confidence\n 4 Kings             74692           1.27 High Confidence\n 5 Monroe            71450           1.35 High Confidence\n 6 Nassau           137709           1.39 High Confidence\n 7 Westchester      114651           1.56 High Confidence\n 8 Onondaga          71479           1.57 High Confidence\n 9 New York          99880           1.78 High Confidence\n10 Bronx             47036           1.89 High Confidence\n# ℹ 52 more rows\n\n\n\n# ---- 3.1: pick counties for tract-level study ----\nselected_counties &lt;- county_reliability %&gt;%\n  dplyr::filter(county %in% c(\"Kings\", \"Greene\", \"Hamilton\")) %&gt;%\n  dplyr::select(county, GEOID, med_incomeE, income_moe_pct, reliability)\n\nknitr::kable(\n  selected_counties,\n  caption = \"Selected counties for tract-level analysis\"\n)\n\n\nSelected counties for tract-level analysis\n\n\ncounty\nGEOID\nmed_incomeE\nincome_moe_pct\nreliability\n\n\n\n\nGreene\n36039\n70294\n6.175491\nModerate Confidence\n\n\nHamilton\n36041\n66891\n11.394657\nLow Confidence\n\n\nKings\n36047\n74692\n1.266535\nHigh Confidence\n\n\n\n\n\nComment on the output: Only one county in New York is low confidence with regard to MOE - Hamilton, NY. This is likely due to extremely small population size coupled with small sample perhaps."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#tract-level-demographics",
    "href": "Assignments/Assignment_1/assignment_1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# ---- 3.2: Tract-level demographics (robust county naming via FIPS) ----\n\n# 1) Variables\nvars_tracts &lt;- c(\n  white_nh  = \"B03002_003\",  # White alone, not Hispanic or Latino\n  black_nh  = \"B03002_004\",  # Black alone, not Hispanic or Latino\n  hispanic  = \"B03002_012\",  # Hispanic or Latino\n  total_pop = \"B03002_001\"   # Total population\n)\n\n# 2) 3-digit county FIPS from your selected counties (5-digit GEOID -&gt; last 3)\ncounty_codes3 &lt;- stringr::str_sub(selected_counties$GEOID, -3, -1) %&gt;% unique()\n\n# 3) Pull ACS tract-level data\ntract_raw &lt;- tidycensus::get_acs(\n  geography = \"tract\",\n  variables = vars_tracts,\n  year      = acs_year,\n  survey    = \"acs5\",\n  state     = my_state,\n  county    = county_codes3,\n  output    = \"wide\"\n)\n\n# 4) Derive state/county FIPS from the tract GEOID and join to official names\n#    tract GEOID = SS + CCC + TTTTTT  (state 2, county 3, tract 6)\ntract_demo &lt;- tract_raw %&gt;%\n  dplyr::mutate(\n    state_fips  = stringr::str_sub(GEOID, 1, 2),\n    county_fips = stringr::str_sub(GEOID, 3, 5),\n    tract_label = stringr::str_extract(NAME, \"^Census Tract\\\\s*[^,]+\") %&gt;%\n                  stringr::str_remove(\"^Census Tract\\\\s*\")\n  ) %&gt;%\n  dplyr::left_join(\n    tidycensus::fips_codes %&gt;%\n      dplyr::transmute(\n        state_fips  = state_code,\n        county_fips = county_code,\n        county_label = county,   # e.g., \"Kings\"\n        state_label  = state     # e.g., \"New York\"\n      ),\n    by = c(\"state_fips\", \"county_fips\")\n  ) %&gt;%\n  # 5) Compute percentages\n  dplyr::mutate(\n    pct_white_nh = dplyr::if_else(total_popE &gt; 0, 100 * white_nhE / total_popE, NA_real_),\n    pct_black_nh = dplyr::if_else(total_popE &gt; 0, 100 * black_nhE / total_popE, NA_real_),\n    pct_hispanic = dplyr::if_else(total_popE &gt; 0, 100 * hispanicE / total_popE, NA_real_)\n  )\n\n# Quick peek\nknitr::kable(\n  tract_demo %&gt;%\n    dplyr::select(\n      GEOID, tract_label, county_label,\n      total_popE, white_nhE, pct_white_nh,\n      black_nhE, pct_black_nh,\n      hispanicE, pct_hispanic\n    ) %&gt;% head(10),\n  caption = \"Tract demographics with population percentages (first 10 rows)\",\n  digits = 1\n)\n\n\nTract demographics with population percentages (first 10 rows)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\ntract_label\ncounty_label\ntotal_popE\nwhite_nhE\npct_white_nh\nblack_nhE\npct_black_nh\nhispanicE\npct_hispanic\n\n\n\n\n36039080100\n801; Greene County; New York\nGreene County\n3230\n2866\n88.7\n59\n1.8\n168\n5.2\n\n\n36039080201\n802.01; Greene County; New York\nGreene County\n3765\n3361\n89.3\n2\n0.1\n204\n5.4\n\n\n36039080202\n802.02; Greene County; New York\nGreene County\n2641\n2048\n77.5\n176\n6.7\n145\n5.5\n\n\n36039080301\n803.01; Greene County; New York\nGreene County\n1611\n1363\n84.6\n42\n2.6\n52\n3.2\n\n\n36039080302\n803.02; Greene County; New York\nGreene County\n1424\n1240\n87.1\n15\n1.1\n78\n5.5\n\n\n36039080402\n804.02; Greene County; New York\nGreene County\n2043\n1582\n77.4\n0\n0.0\n231\n11.3\n\n\n36039080403\n804.03; Greene County; New York\nGreene County\n1273\n1254\n98.5\n0\n0.0\n19\n1.5\n\n\n36039080404\n804.04; Greene County; New York\nGreene County\n1755\n1654\n94.2\n18\n1.0\n71\n4.0\n\n\n36039080501\n805.01; Greene County; New York\nGreene County\n2726\n2551\n93.6\n27\n1.0\n98\n3.6\n\n\n36039080502\n805.02; Greene County; New York\nGreene County\n3967\n3651\n92.0\n0\n0.0\n211\n5.3"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#demographic-analysis",
    "href": "Assignments/Assignment_1/assignment_1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\nFind the tract with the highest percentage of Hispanic/Latino residents Hint: use arrange() and slice() to get the top tract\nCalculate average demographics by county using group_by() and summarize() Show: number of tracts, average percentage for each racial/ethnic group\nCreate a nicely formatted table of your results using kable()\n\n# ---- 3.3A: tract with highest % Hispanic ----\ntop_hispanic_tract &lt;- tract_demo %&gt;%\n  dplyr::filter(!is.na(pct_hispanic)) %&gt;%\n  dplyr::arrange(dplyr::desc(pct_hispanic)) %&gt;%\n  dplyr::slice(1) %&gt;%\n  dplyr::select(\n    GEOID, tract_label, county_label,\n    total_popE, pct_white_nh, pct_black_nh, pct_hispanic\n  )\n\nknitr::kable(\n  top_hispanic_tract,\n  digits = c(0, 0, 0, 0, 1, 1, 1),\n  caption = \"Tract with the highest % Hispanic\"\n)\n\n\nTract with the highest % Hispanic\n\n\n\n\n\n\n\n\n\n\n\nGEOID\ntract_label\ncounty_label\ntotal_popE\npct_white_nh\npct_black_nh\npct_hispanic\n\n\n\n\n36047002000\n20; Kings County; New York\nKings County\n1899\n3.5\n2.5\n79.8\n\n\n\n\n# ---- 3.3B: summarize by county (population-weighted) ----\ncounty_summary_weighted &lt;- tract_demo %&gt;%\n  dplyr::group_by(county_label) %&gt;%\n  dplyr::summarise(\n    total_pop = sum(total_popE, na.rm = TRUE),\n    white_nh  = sum(white_nhE,  na.rm = TRUE),\n    black_nh  = sum(black_nhE,  na.rm = TRUE),\n    hispanic  = sum(hispanicE,  na.rm = TRUE),\n    n_tracts  = dplyr::n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  dplyr::mutate(\n    pct_white_nh = 100 * white_nh / total_pop,\n    pct_black_nh = 100 * black_nh / total_pop,\n    pct_hispanic = 100 * hispanic / total_pop\n  ) %&gt;%\n  dplyr::select(\n    county_label, n_tracts, total_pop,\n    pct_white_nh, pct_black_nh, pct_hispanic\n  )\n\nknitr::kable(\n  county_summary_weighted,\n  digits = c(0, 0, 0, 1, 1, 1),\n  col.names = c(\"County\", \"Tracts\", \"Population\",\n                \"% White (NH)\", \"% Black (NH)\", \"% Hispanic\"),\n  caption = \"County-wide demographics (ACS 2018–2022 5-year, weighted by population)\"\n)\n\n\nCounty-wide demographics (ACS 2018–2022 5-year, weighted by population)\n\n\n\n\n\n\n\n\n\n\nCounty\nTracts\nPopulation\n% White (NH)\n% Black (NH)\n% Hispanic\n\n\n\n\nGreene County\n18\n48067\n83.8\n4.6\n6.5\n\n\nHamilton County\n4\n5090\n92.1\n1.1\n2.0\n\n\nKings County\n805\n2679620\n36.1\n28.3\n18.9"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/Assignment_1/assignment_1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\nCalculate MOE percentages for white, Black, and Hispanic variables Hint: use the same formula as before (margin/estimate * 100)\nCreate a flag for tracts with high MOE on any demographic variable Use logical operators (| for OR) in an ifelse() statement\nCreate summary statistics showing how many tracts have data quality issues\n\n# ---- 4.1: MOE analysis for demographic variables ----\n\n# Safety: ensure tract_demo exists\nstopifnot(exists(\"tract_demo\"))\n\ntract_moe &lt;- tract_demo %&gt;%\n  dplyr::mutate(\n    # MOE% for each group (only when estimate &gt; 0 and MOE available)\n    white_moe_pct    = dplyr::if_else(!is.na(white_nhE)  & white_nhE  &gt; 0 & !is.na(white_nhM),\n                                      100 * white_nhM  / white_nhE,  NA_real_),\n    black_moe_pct    = dplyr::if_else(!is.na(black_nhE)  & black_nhE  &gt; 0 & !is.na(black_nhM),\n                                      100 * black_nhM  / black_nhE,  NA_real_),\n    hispanic_moe_pct = dplyr::if_else(!is.na(hispanicE)  & hispanicE  &gt; 0 & !is.na(hispanicM),\n                                      100 * hispanicM  / hispanicE,  NA_real_),\n\n    # Per-group &gt;15% flags\n    over15_white = white_moe_pct    &gt; 15,\n    over15_black = black_moe_pct    &gt; 15,\n    over15_hisp  = hispanic_moe_pct &gt; 15,\n\n    # Any group over threshold\n    high_moe_flag = over15_white | over15_black | over15_hisp\n  )\n\n# Overall summary\ntract_moe_summary &lt;- tract_moe %&gt;%\n  dplyr::summarise(\n    total_tracts    = dplyr::n(),\n    high_moe_tracts = sum(high_moe_flag, na.rm = TRUE),\n    pct_high_moe    = round(100 * high_moe_tracts / total_tracts, 1),\n    # count tracts where at least one MOE% couldn't be computed\n    any_moe_na      = sum(is.na(white_moe_pct) | is.na(black_moe_pct) | is.na(hispanic_moe_pct))\n  )\n\nknitr::kable(\n  tract_moe_summary,\n  digits = 1,\n  caption = \"Summary of high-MOE tracts (ACS 2018–2022 5-year)\"\n)\n\n\nSummary of high-MOE tracts (ACS 2018–2022 5-year)\n\n\ntotal_tracts\nhigh_moe_tracts\npct_high_moe\nany_moe_na\n\n\n\n\n827\n801\n96.9\n114"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#pattern-analysis",
    "href": "Assignments/Assignment_1/assignment_1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\nGroup tracts by whether they have high MOE issues. Calculate average characteristics for each group:population size, demographic percentages\nUse group_by() and summarize() to create this comparison. Create a professional table showing the pattern\n\n# ---- 4.2A: Compare tracts with vs. without high MOE issues ----\n\npattern_analysis &lt;- tract_moe %&gt;%\n  dplyr::group_by(high_moe_flag) %&gt;%\n  dplyr::summarise(\n    n_tracts         = dplyr::n(),\n    avg_pop          = mean(total_popE, na.rm = TRUE),\n    avg_pct_white    = mean(pct_white_nh, na.rm = TRUE),\n    avg_pct_black    = mean(pct_black_nh, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\nknitr::kable(\n  pattern_analysis,\n  digits = 1,\n  col.names = c(\"High MOE Flag\", \"Tracts\", \"Avg Pop\",\n                \"Avg % White (NH)\", \"Avg % Black (NH)\", \"Avg % Hispanic\"),\n  caption = \"Comparison of tracts with vs. without high MOE issues\"\n)\n\n\nComparison of tracts with vs. without high MOE issues\n\n\n\n\n\n\n\n\n\n\nHigh MOE Flag\nTracts\nAvg Pop\nAvg % White (NH)\nAvg % Black (NH)\nAvg % Hispanic\n\n\n\n\nTRUE\n801\n3411.7\n37.8\n27.4\n17.9\n\n\nNA\n26\n0.0\nNaN\nNaN\nNaN\n\n\n\n\n# ---- 4.2B: Count MOE&gt;15% flags by county ----\n\nflags_by_county &lt;- tract_moe %&gt;%\n  dplyr::group_by(county_label) %&gt;%\n  dplyr::summarise(\n    white_flags    = sum(over15_white,    na.rm = TRUE),\n    black_flags    = sum(over15_black,    na.rm = TRUE),\n    hispanic_flags = sum(over15_hisp,     na.rm = TRUE),\n    total_tracts   = dplyr::n(),\n    any_high_moe   = sum(high_moe_flag,   na.rm = TRUE),\n    pct_flagged    = round(100 * any_high_moe / total_tracts, 1),\n    .groups = \"drop\"\n  ) %&gt;%\n  dplyr::arrange(dplyr::desc(pct_flagged))\n\nknitr::kable(\n  flags_by_county,\n  digits = 1,\n  caption = \"Tracts with MOE% &gt; 15% by county (counts and % of tracts flagged)\"\n)\n\n\nTracts with MOE% &gt; 15% by county (counts and % of tracts flagged)\n\n\n\n\n\n\n\n\n\n\n\ncounty_label\nwhite_flags\nblack_flags\nhispanic_flags\ntotal_tracts\nany_high_moe\npct_flagged\n\n\n\n\nGreene County\n7\n14\n18\n18\n18\n100.0\n\n\nHamilton County\n2\n4\n4\n4\n4\n100.0\n\n\nKings County\n734\n706\n771\n805\n779\n96.8\n\n\n\n\n\nPattern Analysis: [In rural counties like Greene and Hamilton, ACS reliability issues fall disproportionately on minority groups: nearly every Black and Hispanic estimate is flagged as unreliable, while White estimates are more stable. In contrast, in Kings County (Brooklyn), almost every tract shows MOEs above 15% for all racial/ethnic groups (White ≈91%, Black ≈88%, Hispanic ≈96%). This demonstrates that in small-population counties, unreliability is concentrated on underrepresented groups, while in large urban counties the problem is structural to the tract-level ACS — margins of error are high across the board. For policy and algorithmic applications, this means that minority communities in rural areas risk being systematically undercounted, while in urban areas all groups are at risk of noisy estimates that could mislead algorithmic decision-making"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "href": "Assignments/Assignment_1/assignment_1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nThis analysis of American Community Survey (ACS) data for New York counties reveals a striking divergence between county-level reliability and tract-level detail. At the county scale, median household income estimates are generally robust: the majority of counties fall within the “High Confidence” category (MOE &lt; 5%), and even in New York City’s Kings County the median income estimate is highly reliable. However, when we move to tract-level race and ethnicity measures, margins of error grow dramatically. Across the selected counties, nearly all tracts report at least one racial/ethnic estimate with MOE greater than 15%, demonstrating that while income data is dependable for algorithmic policy use, subgroup demographic data is far more uncertain.\nThe communities most at risk of algorithmic bias differ by geography. In rural counties such as Greene and Hamilton, small minority populations produce unstable estimates, leading to systematically unreliable data for Black and Hispanic residents. In contrast, in large urban counties like Kings, data quality issues are not confined to minority groups—margins of error are high for nearly all racial/ethnic categories. Thus, while statewide income thresholds could be applied equitably, any algorithm that targets resources based on racial composition at the tract level risks disproportionately misclassifying both rural minority communities and dense urban neighborhoods.\nThe root causes of these quality issues stem from the ACS’s design. The survey is sample-based, and smaller geographies such as census tracts often have limited sample sizes. This yields stable estimates for broadly distributed characteristics like total population or income, but highly uncertain estimates for smaller subgroups. The result is that variables most relevant to equity and inclusion—race and ethnicity—are precisely those with the least reliable data. This structural limitation increases the risk that automated decision-making systems will entrench inequities rather than reduce them.\nTo mitigate these risks, the Department should adopt a layered strategy. First, continue to use county-level median income data for resource allocation, where confidence is high. Second, treat tract-level demographic estimates with caution: use them only in conjunction with safeguards such as minimum population thresholds or higher-level geographic aggregation. Third, flag tracts where margins of error exceed set thresholds for manual review rather than algorithmic assignment. Finally, pair ACS data with supplemental sources—administrative records, local surveys, or community-based reporting—to ensure that equity-sensitive decisions are not based solely on unreliable estimates. Taken together, these measures will allow the Department to leverage ACS data responsibly while minimizing the risks of algorithmic biase"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#specific-recommendations",
    "href": "Assignments/Assignment_1/assignment_1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# ---- Decision framework table (minimal + robust) ----\nlibrary(dplyr)\nlibrary(knitr)\n\ndecision_tbl &lt;- county_reliability %&gt;%\n  # keep rows where we can actually decide something\n  filter(!is.na(med_incomeE), !is.na(income_moe_pct)) %&gt;%\n  # If your data already has `reliability`, we’ll use it; otherwise derive it from MOE%\n  mutate(\n    reliability = case_when(\n      !is.na(reliability) ~ reliability,\n      income_moe_pct &lt;= 5     ~ \"High Confidence\",\n      income_moe_pct &lt;= 10    ~ \"Moderate Confidence\",\n      TRUE                    ~ \"Low Confidence\"\n    ),\n    # Map reliability -&gt; implementation recommendation\n    recommendation = case_when(\n      reliability == \"High Confidence\"      ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\"  ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\"       ~ \"Requires manual review or additional data\",\n      TRUE                                  ~ \"Review needed\"\n    ),\n    # Pretty formatting for display only\n    `Median income ($)` = formatC(med_incomeE, format = \"f\", digits = 0, big.mark = \",\"),\n    `Income MOE %` = sprintf(\"%.1f%%\", income_moe_pct),\n    # Order rows by reliability (High → Moderate → Low)\n    reliability = factor(reliability, levels = c(\"High Confidence\",\"Moderate Confidence\",\"Low Confidence\"))\n  ) %&gt;%\n  transmute(\n    County = county,\n    `Median income ($)`,\n    `Income MOE %`,\n    Reliability = reliability,\n    Recommendation = recommendation\n  ) %&gt;%\n  arrange(Reliability, County)\n\nknitr::kable(\n  decision_tbl,\n  caption = \"Decision Framework for Algorithm Implementation (ACS 2022 5-year)\"\n)\n\n\nDecision Framework for Algorithm Implementation (ACS 2022 5-year)\n\n\n\n\n\n\n\n\n\nCounty\nMedian income ($)\nIncome MOE %\nReliability\nRecommendation\n\n\n\n\nAlbany\n78,829\n2.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAllegany\n58,725\n3.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBronx\n47,036\n1.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBroome\n58,317\n3.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCattaraugus\n56,889\n3.1%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCayuga\n63,227\n4.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChautauqua\n54,625\n3.2%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChemung\n61,358\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nChenango\n61,741\n4.1%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nClinton\n67,097\n4.2%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColumbia\n81,741\n3.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCortland\n65,029\n4.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDelaware\n58,338\n3.7%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDutchess\n94,578\n2.7%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nErie\n68,014\n1.2%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFranklin\n60,270\n4.8%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFulton\n60,557\n4.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGenesee\n68,178\n4.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHerkimer\n68,104\n4.8%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nJefferson\n62,782\n3.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n74,692\n1.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLewis\n64,401\n4.2%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLivingston\n70,443\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadison\n68,869\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonroe\n71,450\n1.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMontgomery\n58,033\n3.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNassau\n137,709\n1.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNew York\n99,880\n1.8%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNiagara\n65,882\n2.7%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOneida\n66,402\n3.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOnondaga\n71,479\n1.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOntario\n76,603\n2.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n91,806\n1.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrleans\n61,069\n4.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOswego\n65,054\n3.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOtsego\n65,778\n4.5%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPutnam\n120,970\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nQueens\n82,431\n1.1%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRensselaer\n83,734\n2.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRichmond\n96,185\n2.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRockland\n106,173\n2.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSaratoga\n97,038\n2.3%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSchenectady\n75,056\n3.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSchoharie\n71,479\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSt. Lawrence\n58,339\n3.5%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSteuben\n62,506\n2.9%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSuffolk\n122,498\n1.2%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSullivan\n67,841\n4.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTioga\n70,427\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTompkins\n69,995\n4.0%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUlster\n77,197\n4.5%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n74,531\n4.7%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWashington\n68,703\n3.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWayne\n71,007\n3.1%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWestchester\n114,651\n1.6%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWyoming\n65,066\n3.4%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n68,090\n5.3%\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nGreene\n70,294\n6.2%\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSchuyler\n61,316\n9.5%\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSeneca\n64,050\n5.2%\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nYates\n63,974\n5.8%\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHamilton\n66,891\n11.4%\nLow Confidence\nRequires manual review or additional data\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Counties with High Confidence median income data, such as Kings County, are appropriate for algorithmic targeting when the policy is tied to income thresholds. Median household income estimates here have margins of error well under 5%, which provides sufficient reliability for resource allocation decisions. However, even in these counties, subgroup race/ethnicity estimates should not be used as standalone inputs given their high margins of error.\nCounties requiring additional oversight: Counties with Moderate Confidence income estimates, such as Greene County, can be incorporated into algorithmic systems only with safeguards. Algorithms should flag Greene’s tracts for periodic review and be supplemented with monitoring of outcomes to ensure that borderline or unstable estimates do not result in systematic underfunding. Oversight might include comparing ACS estimates with administrative data or community-level surveys on a recurring basis.\nCounties needing alternative approaches: Counties with Low Confidence data, such as Hamilton County, should not be subject to fully automated decision-making. Here, the small population base yields large margins of error that make both income and subgroup data unreliable. Policy in these counties should rely on manual review, qualitative assessments from local agencies, or additional data collection (e.g., oversampling or local needs assessments) before allocating funds"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#questions-for-further-investigation",
    "href": "Assignments/Assignment_1/assignment_1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\nSpatial patterns: Do tracts with high MOE cluster geographically (e.g., in rural versus urban areas), and how might these clusters bias statewide resource allocation?\nTime trends: How stable are ACS estimates over multiple 5-year periods, and could temporal averaging reduce the volatility of tract-level race/ethnicity estimates?\nAlternative data sources: Could administrative records (e.g., SNAP enrollment, school free-lunch participation) provide more reliable indicators of local need than ACS tract-level subgroup data?"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#technical-notes",
    "href": "Assignments/Assignment_1/assignment_1.html#technical-notes",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Technical Notes",
    "text": "Technical Notes\nData Sources:\nU.S. Census Bureau, American Community Survey (ACS) 2018–2022 5-Year Estimates\nData retrieved using the tidycensus R package on 4.5.1\nReproducibility\nAll analysis conducted in R version [insert version, e.g., 4.3.2]\nRequired packages: tidycensus, tidyverse, knitr, stringr\nA valid Census API key is required; results can be replicated by loading the same variables and geographies for ACS 5-year 2022\nComplete code and documentation are available at: [your portfolio URL]\nMethodology Notes\nCounty-level income and population estimates were retrieved in “wide” format to simplify margin of error calculations.\nReliability categories were created using thresholds (&lt;5% = High, 5–10% = Moderate, &gt;10% = Low).\nCounties were selected for tract-level analysis to represent different reliability levels (Kings = high, Greene = moderate, Hamilton = low).\nTract-level race/ethnicity percentages were calculated by dividing group counts by total population, then multiplying by 100.\nCounty labels for tracts were assigned using official FIPS codes rather than string parsing to avoid NA parsing issues.\nHigh MOE flags were set at 15% for subgroup demographics, following common practice in ACS data quality analysis.\nLimitations\nACS estimates are based on sample surveys and are subject to sampling error; margins of error can be especially large for small populations and detailed subgroups.\nTract-level data for race/ethnicity is particularly volatile; nearly all tracts in Kings County showed MOE% &gt; 15%.\nRural counties (e.g., Hamilton) have very small populations, making subgroup estimates unreliable or unavailable.\nTemporal limitations: results are based on the 2018–2022 5-year ACS and may not reflect more recent economic or demographic shifts.\nGeographic limitations: analysis focused on New York; findings may not generalize to other states without adjustment for local context."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment_1.html#submission-checklist",
    "href": "Assignments/Assignment_1/assignment_1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#assignment-overview",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(tidyverse)    \nlibrary(sf)           \nlibrary(tidycensus)   \nlibrary(tigris)"
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\n# Create county-level access map\n# Join county summary data to county spatial boundaries\n# Filter out counties with no vulnerable tracts (to remove gray areas)\ncounties_for_map &lt;- pa_counties_projected %&gt;%\n  left_join(county_summary, by = c(\"NAMELSAD\" = \"NAMELSAD.y\")) %&gt;%\n  mutate(pct_underserved = ifelse(is.na(pct_underserved), 0, pct_underserved))\n\n# Create the map\nggplot() +\n  # County fill by percentage underserved\n  geom_sf(data = counties_for_map, \n          aes(fill = pct_underserved), \n          color = \"white\", \n          size = 0.3) +\n  # Hospital locations as points\n  geom_sf(data = hospitals_projected, \n          color = \"darkblue\", \n          size = 1.2, \n          alpha = 0.7,\n          shape = 3) +  # Shape 3 = plus sign (cross)\n  # Color scheme\n  scale_fill_gradient(low = \"lightyellow\", \n                      high = \"darkred\",\n                      name = \"% Underserved\") +\n  # Labels and theme\n  labs(title = \"Healthcare Access for Vulnerable Populations in Pennsylvania\",\n       subtitle = \"Percentage of vulnerable tracts underserved by county (+ symbols = hospitals)\",\n       caption = \"Source: 2022 ACS 5-Year Estimates, PA Hospital Data\\nVulnerable = low income (bottom 25%) + high elderly (top 25%)\\nUnderserved = &gt;15 miles from nearest hospital\") +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11),\n    plot.caption = element_text(size = 8, hjust = 0),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nThe above map shows the percent of vulnerable tracts that are underserved and the location of hospitals marked by the dark blue “+” symbols per the assignments instructions. As can be seen, these large counties have very few hospitals, with a 3 of them having no hospitals at all.\nHowever, the map is somewhat misleading. Many of these counties only have 1 tract that is vulnerable. This map makes could be misleading in making it seem like these whole counties are vulnerable and underserved.\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Map 2 with bolder vulnerable tracts\nggplot() +\n  geom_sf(data = pa_counties_projected, \n          fill = \"gray95\", \n          color = \"gray60\", \n          size = 0.5) +\n  geom_sf(data = vulnerable_tracts_projected %&gt;% filter(vulnerable == TRUE, underserved == FALSE),\n          fill = \"skyblue\",\n          color = \"blue\",  # Add outline\n          size = 0.5,\n          alpha = 0.7) +\n  geom_sf(data = vulnerable_tracts_projected %&gt;% filter(underserved == TRUE),\n          fill = \"darkred\",\n          color = \"red\",\n          size = 0.7,\n          alpha = 0.9) +\n  geom_sf(data = hospitals_projected,\n          color = \"darkgreen\",\n          size = 1.5,\n          alpha = 0.8,\n          shape = 3) +\n  labs(title = \"Underserved Vulnerable Populations in Pennsylvania\",\n       subtitle = \"Red = underserved vulnerable tracts (&gt;15 mi from hospital) | Blue = other vulnerable tracts | + = Hospitals\",\n       caption = \"Source: 2022 ACS 5-Year Estimates, PA Hospital Data\") +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10),\n    plot.caption = element_text(size = 8, hjust = 0)\n  )\n\n\n\n\n\n\n\n\nThe above map shows vulnerable tracts in light blue with a dark boarder, and vulnerable underserved tracts in dark red. As can be seen, none of the dark red tracts have a hospital in them. For Philadelphia, despite having many hospitals, there are still the highest number of vulnerable tracts.\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\n\n# Create histogram showing distribution of distances to hospitals for vulnerable populations\nggplot(vulnerable_tracts_projected %&gt;% filter(vulnerable == TRUE), \n       aes(x = dist_to_hospital_mi)) +\n  geom_histogram(bins = 20, \n                 fill = \"steelblue\", \n                 color = \"white\", \n                 alpha = 0.8) +\n  geom_vline(xintercept = 15, \n             linetype = \"dashed\", \n             color = \"red\", \n             size = 1) +\n  annotate(\"text\", \n           x = 15.5, \n           y = Inf, \n           label = \"15 mile threshold\", \n           vjust = 1.5, \n           hjust = 0, \n           color = \"red\") +\n  labs(title = \"Distribution of Distance to Nearest Hospital for Vulnerable Tracts\",\n       x = \"Distance to Nearest Hospital (miles)\",\n       y = \"Number of Tracts\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nThe histogram above shows that a majority of vulnerable populations live within 10 miles of hospitals. With only a few living more than 15 miles away from a hospital, qualifying them as underserved. These populations are at a very high level of risk, but many other factors in addition to distance from a hospital can put vulnerable populations at risk. Of the top 10 counties with the highest vulnerable populations, one of the counties - Clearfield - has a tract that that is also underserved.\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment —\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies —\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load recreation centers dataset\nrec_centers &lt;- st_read(\"data/PPR_Program_Sites.geojson\")\n\nReading layer `PPR_Program_Sites' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Assignments\\Assignment_2\\data\\PPR_Program_Sites.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 171 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2563 ymin: 39.90444 xmax: -74.96944 ymax: 40.12284\nGeodetic CRS:  WGS 84\n\n# Check basic information\nprint(paste(\"Number of recreation centers:\", nrow(rec_centers)))\n\n[1] \"Number of recreation centers: 171\"\n\nprint(paste(\"CRS:\", st_crs(rec_centers)$input))\n\n[1] \"CRS: WGS 84\"\n\n# Check what columns we have\nnames(rec_centers)\n\n [1] \"OBJECTID\"     \"PARK_NAME\"    \"DPP_ASSET_ID\" \"PROGRAM_TYPE\" \"SITE_CLASS\"  \n [6] \"BUILDING\"     \"GYM\"          \"LABEL_NUMBER\" \"COMMENTS\"     \"DATA_SOURCE\" \n[11] \"geometry\"    \n\n# View first few rows to understand the data\nhead(rec_centers)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2563 ymin: 39.96572 xmax: -75.16713 ymax: 39.97446\nGeodetic CRS:  WGS 84\n  OBJECTID                          PARK_NAME DPP_ASSET_ID PROGRAM_TYPE\n1      807 Tiffany Fletcher Recreation Center         1911      PPR_REC\n2      808        Roberto Clemente Playground         1831      PPR_REC\n3      809              Miles Mack Playground         1910      PPR_REC\n4      810      William T Granahan Playground         1864      PPR_REC\n5      811     Francisville Recreation Center         1859      PPR_REC\n6      812          Charles A Papa Playground         1929      PPR_REC\n  SITE_CLASS BUILDING GYM LABEL_NUMBER\n1          A        Y   N         &lt;NA&gt;\n2          B        Y   N         &lt;NA&gt;\n3          A        Y   N         &lt;NA&gt;\n4          B        Y   N         &lt;NA&gt;\n5          A        Y   N         &lt;NA&gt;\n6          A        Y   N         &lt;NA&gt;\n                                                            COMMENTS\n1 Former name Mill Creek Playground and Recreation Center (12/2022).\n2                                                                   \n3                                                                   \n4                                    Located within Cobbs Creek Park\n5                                                                   \n6                                            Located in Morris Park.\n     DATA_SOURCE                   geometry\n1 Programs 11/24 POINT (-75.21582 39.96598)\n2 Programs 11/24 POINT (-75.16794 39.96572)\n3 Programs 11/24 POINT (-75.19562 39.96748)\n4 Programs 11/24  POINT (-75.2504 39.96936)\n5 Programs 11/24  POINT (-75.16713 39.9685)\n6 Programs 11/24  POINT (-75.2563 39.97446)\n\n\nI chose the Philadelphia Parks & Recreation (PPR) Program Sites dataset. This dataset contains recreation centers, playgrounds, and other youth-focused facilities operated by Philadelphia Parks & Recreation. I selected this dataset because recreation centers provide critical after-school programming, sports facilities, and community activities for children and youth, making them an important complement to the hospital access analysis from Part 1. This analysis will examine whether vulnerable populations (low-income elderly) live in areas that also lack youth educational and recreational resources.\nData source: OpenDataPhilly (Philadelphia Parks & Recreation Department). The dataset was last updated in November 2024 according to the “Programs 11/24” notation in the data source field.\nThe dataset contains 171 recreation facilities across Philadelphia.\nThe data is in WGS 84 (EPSG:4326), a geographic coordinate system. We will need to transform it (see below) to Pennsylvania South State Plane (EPSG:3365) to match the projected coordinate system used in Part 1 and enable accurate distance calculations in miles.\n\n# Transform recreation centers to Pennsylvania South State Plane (EPSG:3365)\nrec_centers_projected &lt;- rec_centers %&gt;%\n  st_transform(crs = 3365)\n\n# Check that transformation worked\nprint(paste(\"Recreation centers CRS:\", st_crs(rec_centers_projected)$input))\n\n[1] \"Recreation centers CRS: EPSG:3365\"\n\n\n\n\nResearch question\n\nDo vulnerable populations in Philadelphia have adequate access to recreation centers compared to non-vulnerable populations?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\n\n# Filter to Philadelphia County only using GEOID\n# Philadelphia County FIPS code is 42101 (42 = PA, 101 = Philadelphia)\nphilly_tracts &lt;- pa_tracts_with_data %&gt;%\n  filter(str_starts(GEOID, \"42101\")) %&gt;%  # Filter for Philadelphia County\n  st_transform(crs = 3365) %&gt;%\n  mutate(\n    pct_elderly = (pop_65_plus_total / total_popE) * 100,\n    low_income = median_incomeE &lt; 55924,\n    high_elderly = pct_elderly &gt;= 20.1,\n    vulnerable = low_income & high_elderly\n  ) %&gt;%\n  filter(!is.na(median_incomeE))\n\n# Check how many tracts we got\nprint(paste(\"Philadelphia tracts:\", nrow(philly_tracts)))\n\n[1] \"Philadelphia tracts: 383\"\n\nprint(paste(\"Vulnerable tracts:\", sum(philly_tracts$vulnerable, na.rm = TRUE)))\n\n[1] \"Vulnerable tracts: 17\"\n\nprint(paste(\"Non-vulnerable tracts:\", sum(!philly_tracts$vulnerable, na.rm = TRUE)))\n\n[1] \"Non-vulnerable tracts: 366\"\n\n\n\n# Calculate distance for all Philadelphia tracts\nphilly_centroids &lt;- st_centroid(philly_tracts)\ndistances_to_rec &lt;- st_distance(philly_centroids, rec_centers_projected)\n\nphilly_tracts &lt;- philly_tracts %&gt;%\n  mutate(\n    dist_to_rec_ft = as.numeric(apply(distances_to_rec, 1, min)),\n    dist_to_rec_mi = dist_to_rec_ft / 5280\n  )\n\n\n# Summary statistics by vulnerability status\naccess_comparison &lt;- philly_tracts %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(vulnerable) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_dist_to_rec = mean(dist_to_rec_mi, na.rm = TRUE),\n    median_dist_to_rec = median(dist_to_rec_mi, na.rm = TRUE),\n    pct_within_half_mile = sum(dist_to_rec_mi &lt;= 0.5, na.rm = TRUE) / n() * 100\n  )\n\nprint(access_comparison)\n\n# A tibble: 2 × 5\n  vulnerable num_tracts avg_dist_to_rec median_dist_to_rec pct_within_half_mile\n  &lt;lgl&gt;           &lt;int&gt;           &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1 FALSE             366           0.371              0.335                 80.1\n2 TRUE               17           0.433              0.366                 70.6\n\n\n\n# Map showing distance colored, with vulnerable tracts outlined\nggplot() +\n  # All tracts colored by distance\n  geom_sf(data = philly_tracts,\n          aes(fill = dist_to_rec_mi),\n          color = \"white\",\n          size = 0.2) +\n  # Add thick border around vulnerable tracts\n  geom_sf(data = philly_tracts %&gt;% filter(vulnerable == TRUE),\n          fill = NA,  # No fill, just outline\n          color = \"red\",\n          size = 1.2) +  # Thick red outline\n  # Recreation centers - smaller triangles\n  geom_sf(data = rec_centers_projected,\n          color = \"darkgreen\",\n          size = 1,  # Smaller size\n          shape = 17) +\n  scale_fill_gradient(low = \"lightblue\", high = \"orange\",\n                      name = \"Distance (mi)\\nto Rec Center\") +\n  labs(title = \"Recreation Center Access in Philadelphia: Vulnerable vs Non-Vulnerable Tracts\",\n       subtitle = \"Red outline = Vulnerable tracts | Green triangles = Recreation Centers\",\n       caption = \"Source: Philadelphia Parks & Recreation, 2022 ACS\\nVulnerable = low income + high elderly population\") +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\"),\n    plot.subtitle = element_text(size = 10),\n    plot.caption = element_text(size = 8, hjust = 0),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nInterpretation:\nThe analysis reveals modest disparities in recreation center access between vulnerable and non-vulnerable populations in Philadelphia. Vulnerable tracts are located an average of 0.43 miles from the nearest recreation center compared to 0.37 miles for non-vulnerable tracts, and only 70.6% of vulnerable tracts fall within a half-mile walking distance compared to 80.1% of non-vulnerable tracts. While these differences suggest that vulnerable populations may have slightly worse access to recreational facilities, further statistical testing would be necessary to determine whether these observed differences are statistically significant or could be attributed to random chance. A permutation test or similar randomization procedure would help establish whether the 0.06-mile average difference and the 9.5 percentage point gap in coverage represent meaningful disparities requiring policy intervention. Overall, both groups demonstrate relatively good access to recreation centers, with most Philadelphia tracts located within comfortable walking distance of these facilities."
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nThis time around I tried to format a little better and delete extraneous lines, such as “Your Task” and various instructions. I kept some of the instructions for ease of review as necessary."
  },
  {
    "objectID": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#submission-requirements",
    "href": "Assignments/Assignment_2/Sen_Sam_Assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week-5-notes",
    "section": "",
    "text": "Linear Regression - Parametric Methods (just a few parameters - linear) - Deep learning is actually parametric. just has lots of parameters for every variable - Non parametric (will try and fund best form to find it … not necessarily linear)\nBuilding a model - lm(median_incomeE ~ total_popE, data = padata) -\nStatistical Significance - different samples will produce different lines. If you took lots of samples you’d get different regression lines. Goal is to figure out how representative your line is - Slope = .02 - could we get .02 if the H0 (null hypthesis - no relationship) were true? - we can answer this in 2 ways - t-statistic: how many standard errors away from 0 slope are you. You’re seeing how many SEs you’re away from a 0 relationship slope…bigger T is better (looking for numbers greater than 2) - p value: probability of seeing our estimate if H0 is true\nHow good is model -Inference: look at R^2 (how much of variance is explained by model) -Prediction: R^2 is not enough. How well would it predict new data.\nProblems in Regression for prediction 1) Underfitting - ignores relationshp 2) Good fit - captures true pattern, but still error 3) overfitting - non-paramteric … will not apply well to next sample\nTrain/Test Split 1) Training set - run the regression on the training data set (generally 70%) 2) Test set - see how well your model works on the testing set (generally 30%)\nEvaluate Predictions - RSME -&gt; (Estimate - True value)^2 … if RMSE is $9,500 then it means our predictions are off by ~$9,500 on average … penalizes bigger values - MAE –&gt; estimate - true value\nCross-Validation - Multiple Train/Test splits - K-fold validation (k number of folds) - EG) 10-fold validation. You break up into 10 groups. 9 are train, and then test on 1. Do this 10 times, where every “fold” is tested\nChecking Assumptions - Linear regression assumptions: - 1. Relationship is Linear. - How to check: Residual Plot (want randomness)\n- 2. Constant variance \n- How to check: residual plot (SEs should be normal)\n- how to fix: may be missing variables if you are seeing non-normal residuals \n- Formal Test: Breusch-Pagan --&gt; you want high p-value\n\n- 3. No multicollinearity \n- How to check: Variance inflation factor ... vif(model). \n    - VIF shouldn't be higher than 10 \n\n- 4. No Influential Outliers \n    - What to do: remove if high leverage AND large residuals (cook's distance)\n    - Also: create features instead of \n-\nif violated:\n- coefficients may be biased - Standard errors are wrong - predictions unrealiable"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week-2-notes",
    "section": "",
    "text": "Algorithmic Decision Making & Census Data\nInputs: Synonyms: Independent variables, “x”’s, predictors, features\nOutputs: Synonyms: dependent variable\nGovernment\n-            Idea is that using data will result in more consistent and unbiased output\n-            Reasons for use: Efficiency, consistency , objectivity, cost savings\n1)       Data Science\n2)       Data Analytics (MUSA)\n3)       Machine Learning \n4)       AI\nSubjectiveness\n-            Data cleaning decisions\n-            Data coding or classification\n-            Collection\n-            How you interpret results\n-            What variables you use\nCensus Data Foundations\n·       9 basic questions (age, race, sex, etc.)\n·       Everyone counted every 10 years\n·       Constitutional requirement\n·       Determines political representation\nAmerican Community Survey (ACS)\n·       3% of households surveyed annually\n·       detailed questions (income, education, housing costs, etc)\no   replacement of the same “long form” in 2005\n·       Areas: of only 65,000 people.\n·       Pretty small sample but you get them every year. Only aggregate level\n·       5-year estimates\no   Take all 1-year and combine together including census tracts (1500-1800 people). Most accurate we can get at the smallest geography and we consider them a neighborhood.\n§  It is changed for redistricting\n§  Frustrating for us when redistricting happens\no   Most reliable data, largest sample\nTidycensus\n-            This package helps organize the census data"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(here(\"data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndistricts &lt;- st_read(here(\"data/districts.geojson\"))\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\nhospitals &lt;- st_read(here(\"data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#setup",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#setup",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(here(\"data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndistricts &lt;- st_read(here(\"data/districts.geojson\"))\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\nhospitals &lt;- st_read(here(\"data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `C:\\Users\\16468\\OneDrive - PennO365\\Documents\\Academics\\MUSA\\Public_Policy_Analytics\\portfolio-setup-ssen-droid\\Labs\\Week 4\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 1: Find Your County’s Neighbors (10 minutes)",
    "text": "Exercise 1: Find Your County’s Neighbors (10 minutes)\nGoal: Practice spatial filtering with different predicates\n\n1.1 Pick a Pennsylvania County\nYour Task: Choose any PA county and find all counties that border it.\n\n# Step 1: Look at available county names\nunique(pa_counties$COUNTY_NAM)\n\n [1] \"MONTGOMERY\"     \"BRADFORD\"       \"BUCKS\"          \"TIOGA\"         \n [5] \"UNION\"          \"VENANGO\"        \"WASHINGTON\"     \"WAYNE\"         \n [9] \"MCKEAN\"         \"MERCER\"         \"MIFFLIN\"        \"MONTOUR\"       \n[13] \"NORTHAMPTON\"    \"NORTHUMBERLAND\" \"PERRY\"          \"PIKE\"          \n[17] \"POTTER\"         \"SCHUYLKILL\"     \"SNYDER\"         \"SOMERSET\"      \n[21] \"SULLIVAN\"       \"LEBANON\"        \"BUTLER\"         \"CAMBRIA\"       \n[25] \"CAMERON\"        \"CARBON\"         \"CENTRE\"         \"CLARION\"       \n[29] \"CLEARFIELD\"     \"CLINTON\"        \"COLUMBIA\"       \"CRAWFORD\"      \n[33] \"CUMBERLAND\"     \"DAUPHIN\"        \"INDIANA\"        \"JEFFERSON\"     \n[37] \"JUNIATA\"        \"LANCASTER\"      \"WESTMORELAND\"   \"WYOMING\"       \n[41] \"YORK\"           \"PHILADELPHIA\"   \"LEHIGH\"         \"LUZERNE\"       \n[45] \"LYCOMING\"       \"LAWRENCE\"       \"DELAWARE\"       \"ELK\"           \n[49] \"ERIE\"           \"FAYETTE\"        \"FOREST\"         \"FRANKLIN\"      \n[53] \"FULTON\"         \"GREENE\"         \"HUNTINGDON\"     \"ADAMS\"         \n[57] \"ALLEGHENY\"      \"ARMSTRONG\"      \"BEAVER\"         \"BEDFORD\"       \n[61] \"BLAIR\"          \"SUSQUEHANNA\"    \"WARREN\"         \"BERKS\"         \n[65] \"CHESTER\"        \"MONROE\"         \"LACKAWANNA\"    \n\n# Step 2: Pick one county (change this to your choice!)\nmy_county &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"CENTRE\")  # Change \"CENTRE\" to your county\n\n# Step 3: Find neighbors using st_touches\nmy_neighbors &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_touches)\n\n# Step 4: How many neighbors does your county have?\ncat(\"Number of neighboring counties:\", nrow(my_neighbors), \"\\n\")\n\nNumber of neighboring counties: 6 \n\nprint(\"Neighbor names:\")\n\n[1] \"Neighbor names:\"\n\nprint(my_neighbors$COUNTY_NAM)\n\n[1] \"UNION\"      \"MIFFLIN\"    \"CLEARFIELD\" \"CLINTON\"    \"HUNTINGDON\"\n[6] \"BLAIR\"     \n\n\n\n\n1.2 Map Your Results\nYour Task: Create a map showing your county and its neighbors in different colors.\n\n# Create the map\nggplot() +\n  geom_sf(data = pa_counties, fill = \"lightgray\", color = \"white\") +\n  geom_sf(data = my_neighbors, fill = \"lightblue\", alpha = 0.7) +\n  geom_sf(data = my_county, fill = \"darkblue\") +\n  labs(\n    title = paste(\"Neighbors of\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = paste(nrow(my_neighbors), \"neighboring counties\")\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n1.3 Challenge: Compare with st_intersects\nYour Task: What happens if you use st_intersects instead of st_touches? Why is the count different?\n\n# Use st_intersects\nintersecting_counties &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_intersects)\n\ncat(\"With st_touches:\", nrow(my_neighbors), \"counties\\n\")\n\nWith st_touches: 6 counties\n\ncat(\"With st_intersects:\", nrow(intersecting_counties), \"counties\\n\")\n\nWith st_intersects: 7 counties\n\ncat(\"Difference:\", nrow(intersecting_counties) - nrow(my_neighbors), \"\\n\")\n\nDifference: 1 \n\n\nQuestion: Why is there a difference of 1? What does this tell you about the difference between st_touches and st_intersects?"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-2-hospital-service-areas-15-minutes",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-2-hospital-service-areas-15-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 2: Hospital Service Areas (15 minutes)",
    "text": "Exercise 2: Hospital Service Areas (15 minutes)\nGoal: Practice buffering and measuring accessibility\n\n2.1 Create Hospital Service Areas\nYour Task: Create 15-mile (24140 meter) service areas around all hospitals in your county.\n\n# Step 1: Filter hospitals in your county\n# First do a spatial join to assign counties to hospitals\nhospitals_with_county &lt;- hospitals %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Filter for your county's hospitals\nmy_county_hospitals &lt;- hospitals_with_county %&gt;%\n  filter(COUNTY_NAM == \"CENTRE\")  # Change to match your county\n\ncat(\"Number of hospitals in county:\", nrow(my_county_hospitals), \"\\n\")\n\nNumber of hospitals in county: 3 \n\n# Step 2: Project to accurate CRS for buffering\nmy_county_hospitals_proj &lt;- my_county_hospitals %&gt;%\n  st_transform(3365)  # Pennsylvania State Plane South\n\n# Step 3: Create 15-mile buffers (24140 meters = 15 miles)\nhospital_service_areas &lt;- my_county_hospitals_proj %&gt;%\n  st_buffer(dist = 79200)  # 15 miles in feet for PA State Plane\n\n# Step 4: Transform back for mapping\nhospital_service_areas &lt;- st_transform(hospital_service_areas, st_crs(pa_counties))\n\n\n\n2.2 Map Service Coverage\nYour Task: Create a map showing hospitals and their service areas.\n\nggplot() +\n  geom_sf(data = my_county, fill = \"white\", color = \"gray\") +\n  geom_sf(data = hospital_service_areas, fill = \"lightblue\", alpha = 0.4) +\n  geom_sf(data = my_county_hospitals, color = \"red\", size = 2) +\n  labs(\n    title = paste(\"Hospital Service Areas in\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = \"Red points = Hospitals, Blue areas = 15-mile service zones\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n2.3 Calculate Coverage\nYour Task: What percentage of your county is within 15 miles of a hospital?\n\n# Union all service areas into one polygon\ncombined_service_area &lt;- hospital_service_areas %&gt;%\n  st_union()\n\n# Calculate areas (need to be in projected CRS)\nmy_county_proj &lt;- st_transform(my_county, 3365)\ncombined_service_proj &lt;- st_transform(combined_service_area, 3365)\n\n# Find intersection\ncoverage_area &lt;- st_intersection(my_county_proj, combined_service_proj)\n\n# Calculate percentages\ncounty_area &lt;- as.numeric(st_area(my_county_proj))\ncovered_area &lt;- as.numeric(st_area(coverage_area))\ncoverage_pct &lt;- (covered_area / county_area) * 100\n\ncat(\"County area:\", round(county_area / 1e6, 1), \"sq km\\n\")\n\nCounty area: 30993.6 sq km\n\ncat(\"Covered area:\", round(covered_area / 1e6, 1), \"sq km\\n\")\n\nCovered area: 19205.1 sq km\n\ncat(\"Coverage:\", round(coverage_pct, 1), \"%\\n\")\n\nCoverage: 62 %\n\n\nQuestion: Is your county well-served by hospitals? What areas might be underserved?"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-3-congressional-district-analysis-15-minutes",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-3-congressional-district-analysis-15-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 3: Congressional District Analysis (15 minutes)",
    "text": "Exercise 3: Congressional District Analysis (15 minutes)\nGoal: Practice spatial joins and aggregation\n\n4.1 Join Districts to Counties\nYour Task: Figure out which congressional districts overlap with each county.\n\n# Spatial join: districts to counties\ndistricts_by_county &lt;- districts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_districts = n_distinct(OBJECTID),\n    district_ids = paste(unique(MSLINK), collapse = \", \")\n  ) %&gt;%\n  arrange(desc(n_districts))\n\n# Which counties have the most districts?\nhead(districts_by_county, 10)\n\n# A tibble: 10 × 3\n   COUNTY_NAM   n_districts district_ids            \n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                   \n 1 MONTGOMERY             7 19, 2, 14, 20, 4, 15, 21\n 2 BERKS                  6 8, 19, 2, 14, 4, 5      \n 3 ALLEGHENY              5 11, 12, 17, 1, 7        \n 4 PHILADELPHIA           5 19, 14, 20, 15, 21      \n 5 WESTMORELAND           5 11, 12, 17, 3, 7        \n 6 BUCKS                  4 19, 2, 14, 20           \n 7 CHESTER                4 14, 4, 5, 15            \n 8 DAUPHIN                4 8, 3, 5, 6              \n 9 JUNIATA                4 8, 12, 3, 6             \n10 LANCASTER              4 8, 4, 5, 6              \n\n\n\n\n4.2 Calculate District Statistics\nYour Task: Get demographic data for census tracts and aggregate to districts.\n\n# Get tract-level demographics\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    white_pop = \"B03002_003\",\n    black_pop = \"B03002_004\",\n    hispanic_pop = \"B03002_012\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(tract_demographics, by = \"GEOID\")\n\n# Spatial join to districts and aggregate\ndistrict_demographics &lt;- tracts_with_data %&gt;%\n  st_join(districts) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(OBJECTID, MSLINK) %&gt;%\n  summarize(\n    total_population = sum(total_popE, na.rm = TRUE),\n    median_income = weighted.mean(median_incomeE, total_popE, na.rm = TRUE),\n    pct_white = sum(white_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_black = sum(black_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_hispanic = sum(hispanic_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    n_tracts = n()\n  ) %&gt;%\n  arrange(desc(total_population))\n\n# Show results\nhead(district_demographics, 10)\n\n# A tibble: 10 × 8\n# Groups:   OBJECTID [10]\n   OBJECTID MSLINK total_population median_income pct_white pct_black\n      &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1      113     14          1229246       107602.      72.0     11.0 \n 2      108     11          1010001        76612.      76.6     13.4 \n 3      120      7          1006212        88796.      83.3      8.15\n 4      107      8           993966        70241.      88.7      2.24\n 5      117      3           986114        69017.      91.0      2.25\n 6      109     12           979419        63952.      92.2      1.81\n 7      118      4           974715       111250.      73.0      5.00\n 8      114     17           972225        71139.      91.8      2.75\n 9      110     19           932212       113798.      80.7      3.79\n10      123      6           927718        80080.      75.6      8.65\n# ℹ 2 more variables: pct_hispanic &lt;dbl&gt;, n_tracts &lt;int&gt;\n\n\n\n\n4.3 Map District Demographics\nYour Task: Create a choropleth map of median income by congressional district.\n\n# Join demographics back to district boundaries\ndistricts_with_demographics &lt;- districts %&gt;%\n  left_join(district_demographics, by = \"OBJECTID\")\n\n# Create the map\nggplot(districts_with_demographics) +\n  geom_sf(aes(fill = median_income), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Median\\nIncome\",\n    labels = dollar,\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"Median Household Income by Congressional District\",\n    subtitle = \"Pennsylvania\",\n    caption = \"Source: ACS 2018-2022\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n4.4 Challenge: Find Diverse Districts\nYour Task: Which districts are the most racially diverse?\n\n# Calculate diversity index (simple version: higher = more diverse)\n# A perfectly even distribution would be ~33% each for 3 groups\ndistrict_demographics &lt;- district_demographics %&gt;%\n  mutate(\n    diversity_score = 100 - abs(pct_white - 33.3) - abs(pct_black - 33.3) - abs(pct_hispanic - 33.3)\n  ) %&gt;%\n  arrange(desc(diversity_score))\n\n# Most diverse districts\nhead(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      115     20      37.9     25.4         23.7             77.8\n2      122     21      33.5     49.7          5.73            55.9\n3      121     15      58.8     24.4          5.77            38.1\n4      111      2      71.2      4.88        18.3             18.7\n5      113     14      72.0     11.0          7.09            12.8\n\n# Least diverse districts\ntail(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      107      8      88.7      2.24         5.73           -14.1\n2      116      1      89.3      3.66         2.53           -16.4\n3      117      3      91.0      2.25         3.24           -18.8\n4      114     17      91.8      2.75         1.49           -20.9\n5      109     12      92.2      1.81         2.09           -21.6"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-5-projection-effects-10-minutes",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#exercise-5-projection-effects-10-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 5: Projection Effects (10 minutes)",
    "text": "Exercise 5: Projection Effects (10 minutes)\nGoal: Understand how CRS affects calculations\n\n5.1 Calculate Areas in Different Projections\nYour Task: Calculate county areas using different coordinate systems and compare.\n\n# Calculate areas in different CRS\narea_comparison &lt;- pa_counties %&gt;%\n  # Geographic (WGS84) - WRONG for areas!\n  st_transform(4326) %&gt;%\n  mutate(area_geographic = as.numeric(st_area(.))) %&gt;%\n  # PA State Plane South - Good for PA\n  st_transform(3365) %&gt;%\n  mutate(area_state_plane = as.numeric(st_area(.))) %&gt;%\n  # Albers Equal Area - Good for areas\n  st_transform(5070) %&gt;%\n  mutate(area_albers = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(COUNTY_NAM, starts_with(\"area_\")) %&gt;%\n  mutate(\n    # Calculate errors compared to Albers (most accurate for area)\n    error_geographic_pct = abs(area_geographic - area_albers) / area_albers * 100,\n    error_state_plane_pct = abs(area_state_plane - area_albers) / area_state_plane * 100\n  )\n\n# Show counties with biggest errors\narea_comparison %&gt;%\n  arrange(desc(error_geographic_pct)) %&gt;%\n  select(COUNTY_NAM, error_geographic_pct, error_state_plane_pct) %&gt;%\n  head(10)\n\n    COUNTY_NAM error_geographic_pct error_state_plane_pct\n1         ERIE            0.1520567              90.71567\n2  SUSQUEHANNA            0.1480129              90.71426\n3       MCKEAN            0.1479719              90.71414\n4       WARREN            0.1478826              90.71421\n5     BRADFORD            0.1473177              90.71402\n6        TIOGA            0.1469541              90.71390\n7       POTTER            0.1463317              90.71371\n8     CRAWFORD            0.1447572              90.71326\n9        WAYNE            0.1440222              90.71306\n10     WYOMING            0.1409741              90.71215\n\n\n\n\n5.2 Visualize the Error\nYour Task: Map which counties have the biggest area calculation errors.\n\n# Join error data back to counties\ncounties_with_errors &lt;- pa_counties %&gt;%\n  left_join(\n    area_comparison %&gt;% select(COUNTY_NAM, error_geographic_pct),\n    by = \"COUNTY_NAM\"\n  )\n\n# Map the error\nggplot(counties_with_errors) +\n  geom_sf(aes(fill = error_geographic_pct), color = \"white\") +\n  scale_fill_viridis_c(\n    name = \"Area\\nError %\",\n    option = \"magma\"\n  ) +\n  labs(\n    title = \"Area Calculation Errors by County\",\n    subtitle = \"Using geographic coordinates (WGS84) instead of projected CRS\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nQuestion: Which counties have the largest errors? Why might this be?"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#bonus-challenge-combined-analysis-if-time-permits",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#bonus-challenge-combined-analysis-if-time-permits",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Bonus Challenge: Combined Analysis (If Time Permits)",
    "text": "Bonus Challenge: Combined Analysis (If Time Permits)\nGoal: Combine multiple operations for a complex policy question\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour Task: Combine what you’ve learned to identify vulnerable, underserved communities.\nSteps: 1. Get demographic (elderly and income) data for census tracts 2. Identify vulnerable tracts (low income AND high elderly population) 3. Calculate distance to nearest hospital 4. Check which ones are more than 15 miles from a hospital 5. Aggregate to county level 6. Create comprehensive map 7. Create a summary table\n\n# Your code here!"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#reflection-questions",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#reflection-questions",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Reflection Questions",
    "text": "Reflection Questions\nAfter completing these exercises, reflect on:\n\nWhen did you need to transform CRS? Why was this necessary?\nWhat’s the difference between st_filter() and st_intersection()? When would you use each?\nHow does the choice of predicate (st_touches, st_intersects, st_within) change your results?"
  },
  {
    "objectID": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#summary-of-key-functions-used",
    "href": "Labs/Week 4/week-04/scripts/week4_inclass_practice.html#summary-of-key-functions-used",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Summary of Key Functions Used",
    "text": "Summary of Key Functions Used\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample Use\n\n\n\n\nst_filter()\nSelect features by spatial relationship\nFind neighboring counties\n\n\nst_buffer()\nCreate zones around features\nHospital service areas\n\n\nst_intersects()\nTest spatial overlap\nCheck access to services\n\n\nst_disjoint()\nTest spatial separation\nFind rural areas\n\n\nst_join()\nJoin by location\nAdd county info to tracts\n\n\nst_union()\nCombine geometries\nMerge overlapping buffers\n\n\nst_intersection()\nClip geometries\nCalculate overlap areas\n\n\nst_transform()\nChange CRS\nAccurate distance/area calculations\n\n\nst_area()\nCalculate areas\nCounty sizes, coverage\n\n\nst_distance()\nCalculate distances\nDistance to facilities\n\n\n\nImportant Reminder: Always check and standardize CRS when working with spatial data from multiple sources!"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "RStudio/GitHub overview -PPA is great!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "RStudio/GitHub overview -PPA is great!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nNew R functions or approaches\nQuarto features learned"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNeed to understand the purpose of all the different program and how they speak to each other. For example, I don’t understand how the files that I have clones feed up to my portfolio. How was that connection originally created?"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nN/A"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI am looking forward to building up my technical skills and learning how to code"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week-4-notes",
    "section": "",
    "text": "Simple Features - Simple features - you can have multiple shapes on one row. For example, Hawaii which is multiple columns could be one row of data\n\nyou can add column for geom information in the table.\n\nCoordinate Reference Systems - Making a geographic coordinate system\nStep 1: approximate earth’s shape with an ellipsoid - there are lots of ellipsoid\nStep 2: Tie ellipsoid to the real earth (datum) Example –&gt; Clarke’s meade’s ranch. North American datum of 1927. Also known as NAD 27. also NAD 83 then WS84 are the earth centered instead\n    - All of these are known as geographical earth systems \n    \n    -they are all flat lat/long systems on the 3 dimensional                elispse\n  \nStep 3: Put down you Lat/Long grid on the elipsiod\nStep 4: Project 3D coordinates on computer screen\n-common type is cylindarical projection. if you wrap the earth in an ellipsoid, only the middle is touching the cylinder (line of tangency). Mercator is a famous example\n-the farther away places on the eart are away from the line of tangency in the middle the more distored it is –&gt; good for preserving lattitudal shapes\n-transverse cylindrical –&gt; would be cylinder vertically around the earth and good for preserving longitudal shapes (chile)\n\nconic -&gt; could be good for long countries on the top of the earth (US / China / Russia)\n\nBest way of projeting Projected Coordinate System (this is still starting from an ellipsoid and is then projected using conic or any of the above 1) UTM –&gt; thin zones 2) State plane –&gt; PA –&gt; uses conic projection\nIn R you have to look up the code of the projected coordinate system. all these systems are based on some sort of Datum like NAD 83\nlook up geographic coordinate sysmte vs projected coordinate system\nfor quiz look into path of despair … defining the projection and you need to tell it what it’s in\ndo in class practice"
  }
]